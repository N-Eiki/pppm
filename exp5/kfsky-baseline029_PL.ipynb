{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7zwPKq6MzRs"
   },
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GKfS1UlkMv3T"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f8VWbPE5Mv3X"
   },
   "outputs": [],
   "source": [
    "## transformer をinstall\n",
    "!pip uninstall -y transformers\n",
    "!pip uninstall -y tokenizers\n",
    "!pip install --quiet torch==1.9.1\n",
    "!pip install --quiet transformers==4.16.2\n",
    "!pip install --quiet tokenizers==0.11.6\n",
    "!pip install --quiet sentencepiece\n",
    "!pip install bitsandbytes-cuda112==0.26.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6lypd0rM36v"
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ON1HugHLMv3Z"
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CFG\n",
    "# ====================================================\n",
    "\n",
    "class CFG:\n",
    "  name=\"bigpatent_pl\"\n",
    "  max_len=512 # ★★★\n",
    "  wandb=False\n",
    "  competition=\"PPPM\"\n",
    "  _wandb_kernel=\"kunishou\"\n",
    "  debug=False\n",
    "  apex=True\n",
    "  print_freq=100\n",
    "  num_workers=4\n",
    "  model=\"google/bigbird-pegasus-large-bigpatent\"\n",
    "  #model=\"funnel-transformer/xlarge\"\n",
    "  scheduler=\"CosineAnnealingLR\"\n",
    "  batch_scheduler=True\n",
    "  num_cycles=0.5\n",
    "  num_warmup_steps=50 # change\n",
    "  epochs=5\n",
    "  encoder_lr=2e-5 # change\n",
    "  decoder_lr=2e-5 # change\n",
    "  min_lr=1e-7\n",
    "  eps=5e-6\n",
    "  betas=(0.9, 0.999)\n",
    "  #factor=0.2 # ReduceLROnPlateau\n",
    "  #patience=4 # ReduceLROnPlateau\n",
    "  #eps=1e-6 # ReduceLROnPlateau\n",
    "  T_max=50 # CosineAnnealingLR\n",
    "  #T_0=50 # CosineAnnealingWarmRestarts\n",
    "  batch_size=16   # ★★★ https://www.kaggle.com/c/nbme-score-clinical-patient-notes/discussion/308298\n",
    "  fc_dropout=0.2\n",
    "  weight_decay=0.01\n",
    "  target_size=1\n",
    "  gradient_accumulation_steps=4\n",
    "  max_grad_norm=1000\n",
    "  seed=44\n",
    "  n_fold=5\n",
    "  trn_fold=[0, 1, 2, 3, 4]\n",
    "  train=True\n",
    "  wandb_key = \"\" # not good\n",
    "\n",
    "  # Colab Env\n",
    "  upload_from_colab = True\n",
    "  api_path = \"/content/drive/MyDrive/kaggle/kaggle.json\"\n",
    "  drive_path = \"/content/drive/My Drive/uspppm/\"\n",
    "  \n",
    "  # Kaggle Env\n",
    "  kaggle_dataset_path = None\n",
    "    \n",
    "if CFG.debug:\n",
    "    CFG.epochs = 2\n",
    "    CFG.trn_fold = [0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pc0660AjM7ED"
   },
   "source": [
    "# Import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GXct7puPMv3a"
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import string\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import logging\n",
    "import itertools\n",
    "import datetime\n",
    "import warnings\n",
    "import shutil\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold, StratifiedGroupKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "\n",
    "#!pip uninstall -y transformers\n",
    "#!pip uninstall -y tokenizers\n",
    "#pip install transformers==4.16.2\n",
    "#!pip install tokenizers==0.11.0\n",
    "\n",
    "import tokenizers\n",
    "import transformers\n",
    "print(f\"torch.__version__: {torch.__version__}\")\n",
    "print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n",
    "print(f\"transformers.__version__: {transformers.__version__}\")\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CrBP7FUIMv3c"
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "  # 今回の評価指標\n",
    "  # sp.stats.pearsonrでは tupleで(相関係数, p値)で返ってくるので[0]で値を取得する\n",
    "  # https://www.st-hakky-blog.com/entry/2018/01/30/004659\n",
    "  score = sp.stats.pearsonr(y_true, y_pred)[0]\n",
    "  return score\n",
    "\n",
    "def seed_everything(seed=CFG.seed):\n",
    "  random.seed(seed)\n",
    "  os.environ[\"PYTHONHASHSEED\"] = str(seed) # ハッシュ生成のランダム化を無効\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed) # こっちだけでも一応CUDA側のseedも固定してくれる。複数GPUの場合はmanual_seed_all()\n",
    "  torch.cuda.manual_seed(seed) \n",
    "  torch.backends.cudnn.deterministic=True # 決定論的アルゴリズムを使用する\n",
    "\n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ST3OClAfMv3e"
   },
   "outputs": [],
   "source": [
    "COLAB = \"google.colab\" in sys.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZKPL1h5xMv3f"
   },
   "outputs": [],
   "source": [
    "# Logger\n",
    "class Logger:\n",
    "  def __init__(self, path):\n",
    "    self.general_logger = logging.getLogger(path)\n",
    "    stream_handler = logging.StreamHandler()\n",
    "    file_general_handler = logging.FileHandler(os.path.join(path, \"Experiment.log\"))\n",
    "    if len(self.general_logger.handlers) ==0:\n",
    "      self.general_logger.addHandler(stream_handler)\n",
    "      self.general_logger.addHandler(file_general_handler)\n",
    "      self.general_logger.setLevel(logging.INFO)\n",
    "\n",
    "  def info(self, message):\n",
    "        # display time\n",
    "        self.general_logger.info('[{}] - {}'.format(self.now_string(), message))\n",
    "\n",
    "  @staticmethod\n",
    "  def now_string():\n",
    "      return str(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dDig989zMv3g"
   },
   "outputs": [],
   "source": [
    "# colab環境での設定\n",
    "if COLAB:\n",
    "    #print(\"This environment is Google Colab\")\n",
    "    \n",
    "    # mount\n",
    "    from google.colab import drive\n",
    "    if not os.path.isdir(\"/content/drive\"):\n",
    "        drive.mount('/content/drive') \n",
    "        \n",
    "    #os.chdir('/content/drive/My Drive/uspppm/notebook')\n",
    "\n",
    "    # import library\n",
    "    ! pip install --quiet wandb\n",
    "\n",
    "    # use kaggle api (need kaggle token)\n",
    "    f = open(CFG.api_path, 'r')\n",
    "    json_data = json.load(f) \n",
    "    os.environ[\"KAGGLE_USERNAME\"] = json_data[\"username\"]\n",
    "    os.environ[\"KAGGLE_KEY\"] = json_data[\"key\"]\n",
    "    \n",
    "    # set dirs\n",
    "    DRIVE = CFG.drive_path\n",
    "    EXP = (CFG.name if CFG.name is not None \n",
    "           else get(\"http://172.28.0.2:9000/api/sessions\").json()[0][\"name\"][:-6])\n",
    "    INPUT = os.path.join(DRIVE, \"Input\")\n",
    "    OUTPUT = os.path.join(DRIVE, \"Output\")\n",
    "    # SUBMISSION = os.path.join(DRIVE, \"Submission\")\n",
    "    OUTPUT_EXP = os.path.join(OUTPUT, EXP) \n",
    "    EXP_MODEL = os.path.join(OUTPUT_EXP, \"model\")\n",
    "    # EXP_FIG = os.path.join(OUTPUT_EXP, \"fig\")\n",
    "    EXP_PREDS = os.path.join(OUTPUT_EXP, \"preds\")\n",
    "    EXP_TOKENIZER = os.path.join(OUTPUT_EXP, \"tokenizer\") # change\n",
    "\n",
    "    # make dirs\n",
    "    for d in [INPUT, EXP_MODEL, EXP_PREDS]:\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "\n",
    "    if not os.path.isfile(os.path.join(INPUT, \"train.csv\")):\n",
    "        # load dataset\n",
    "        ! kaggle competitions download -c us-patent-phrase-to-phrase-matching -p $INPUT\n",
    "        # unzip need\n",
    "    \n",
    "    logger = Logger(OUTPUT_EXP)\n",
    "    print(\"This environment is Google Colab\")\n",
    "\n",
    "else:\n",
    "    print(\"This environment is Kaggle Kernel\")\n",
    "    \n",
    "    # set dirs\n",
    "    INPUT = \"../input/us-patent-phrase-to-phrase-matching/\"\n",
    "    EXP, OUTPUT, SUBMISSION = \"./\", \"./\", \"./\"\n",
    "    EXP_MODEL = os.path.join(EXP, \"model\")\n",
    "    # EXP_FIG = os.path.join(EXP, \"fig\")\n",
    "    EXP_PREDS = os.path.join(EXP, \"preds\")\n",
    "    EXP_TOKENIZER = os.path.join(OUTPUT_EXP, \"tokenizer\") # change\n",
    "    \n",
    "    # copy dirs\n",
    "    if CFG.kaggle_dataset_path is not None:\n",
    "        KD_MODEL = os.path.join(CFG.kaggle_dataset_path, \"model\")\n",
    "        KD_EXP_PREDS = os.path.join(CFG.kaggle_dataset_path, \"preds\")\n",
    "        shutil.copytree(KD_MODEL, EXP_MODEL)\n",
    "        shutil.copytree(KD_EXP_PREDS, EXP_PREDS)\n",
    "\n",
    "    # make dirs\n",
    "    for d in [EXP_MODEL, EXP_PREDS]:\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "        \n",
    "    # utils\n",
    "    logger = Logger(OUTPUT_EXP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xqpkp6mxMv3i"
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# wandb\n",
    "# ====================================================\n",
    "if CFG.wandb:\n",
    "  import wandb\n",
    "\n",
    "  try:\n",
    "    wandb.login(key=CFG.wandb_key)\n",
    "    anory = None\n",
    "  except:\n",
    "    anory = \"must\"\n",
    "    print(\"please check wandb key\")\n",
    "\n",
    "  def class2dict(f):\n",
    "    return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n",
    "\n",
    "  run = wandb.init(project=\"PPPM\",\n",
    "                   name=CFG.model,\n",
    "                   config=class2dict(CFG),\n",
    "                   group=CFG.model,\n",
    "                   job_type=\"train\",\n",
    "                   anonymous=anory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xy9mAGtF8d9p"
   },
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UBPE10ANMv3j"
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Data Loading\n",
    "# ====================================================\n",
    "train = pd.read_csv('../input/us-patent-phrase-to-phrase-matching/train_pl2.csv') # ★★★\n",
    "print(f\"train.shape: {train.shape}\")\n",
    "display(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4abIf_RMv3k"
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CPC Data\n",
    "# 追加情報を記載する\n",
    "# ====================================================\n",
    "def get_cpc_texts():\n",
    "  contexts = []\n",
    "  patten = '[A-Z]\\d+'\n",
    "\n",
    "  for file_name in os.listdir(os.path.join(INPUT, 'CPCSchemeXML202105')):\n",
    "    result = re.findall(patten, file_name)\n",
    "    if result:\n",
    "      contexts.append(result)\n",
    "  \"\"\"\n",
    "  入れ子リストをlistに戻す方法\n",
    "  sum(list, [])\n",
    "  \"\"\"\n",
    "  contexts = sorted(set(sum(contexts, [])))\n",
    "  results = {}\n",
    "  for cpc in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'Y']:\n",
    "    with open(os.path.join(INPUT, f'CPCTitleList202202/cpc-section-{cpc}_20220201.txt')) as f:\n",
    "      s = f.read()\n",
    "    pattern = f'{cpc}\\t\\t.+'\n",
    "    result = re.findall(pattern, s)\n",
    "    cpc_result = result[0].lstrip(pattern)\n",
    "    for context in [c for c in contexts if c[0] == cpc]:\n",
    "      pattern = f'{context}\\t\\t.+'\n",
    "      result = re.findall(pattern, s)\n",
    "      results[context] = cpc_result + \". \" + result[0].lstrip(pattern)\n",
    "\n",
    "  return results\n",
    "\n",
    "cpc_texts = get_cpc_texts()    \n",
    "torch.save(cpc_texts, os.path.join(OUTPUT_EXP, \"cpc_texts.pth\"))\n",
    "train['context_text'] = train['context'].map(cpc_texts)\n",
    "display(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_8p8J_wCMv3k"
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# preprocess\n",
    "# https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching/discussion/315827\n",
    "# ====================================================\n",
    "\n",
    "def omit_char(x):\n",
    "  x = x.replace(\";\", \"\") # ; を削除\n",
    "  x = x.replace(\"[\", \"\") # [] を削除\n",
    "  x = x.replace(\"]\", \"\") # [] を削除\n",
    "  x = x.lower() # すべて小文字に変換\n",
    "  return x\n",
    "\n",
    "# 数字除外\n",
    "train['anchor'].replace(\"dry coating composition1\", \"dry coating composition\", inplace=True)\n",
    "\n",
    "train['context_text'] = train['context_text'].map(omit_char)\n",
    "\n",
    "train['context_text'] = train['context_text'].replace(\"human necessities. griculture forestry animal husbandry hunting trapping fishing\", \n",
    "                                                      \"human necessities. agriculture forestry animal husbandry hunting trapping fishing\")\n",
    "\n",
    "train['context_text'] = train['context_text'].str.replace(\"hemistry\", \"chemistry\")\n",
    "\n",
    "display(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a30-o9OGMv3l"
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Create text\n",
    "# ====================================================\n",
    "train['text'] = train['anchor'] + '[SEP]' + train['target'] + '[SEP]'  + train['context_text']\n",
    "\n",
    "display(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4td2VKndDyfK"
   },
   "outputs": [],
   "source": [
    "train['score_map'] = train['score'].map({0.00: 0, 0.25: 1, 0.50: 2, 0.75: 3, 1.00: 4})\n",
    "encoder = LabelEncoder()\n",
    "train['anchor_map'] = encoder.fit_transform(train['anchor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cyzg-yJQMv3l"
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CV split\n",
    "# StratifiedGroupKFoldに変更\n",
    "# ====================================================\n",
    "\n",
    "train_pl = train[train[\"id\"].isnull()].copy().reset_index()\n",
    "train = train[~train[\"id\"].isnull()].copy()\n",
    "\n",
    "Fold = StratifiedGroupKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train, train['score_map'], groups=train[\"anchor_map\"])):\n",
    "  train.loc[val_index, 'fold'] = int(n)\n",
    "\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train_pl, train_pl['score_map'], groups=train_pl[\"anchor_map\"])):\n",
    "  train_pl.loc[val_index, 'fold'] = int(n)\n",
    "\n",
    "train['fold'] = train['fold'].astype(int)\n",
    "train_pl['fold'] = train_pl['fold'].astype(int)\n",
    "\n",
    "display(train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G5j-x_VFFWLc"
   },
   "outputs": [],
   "source": [
    "display(train_pl.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9wVAS1BKFc18"
   },
   "outputs": [],
   "source": [
    "# 別々に5foldに分割したtrain,train_plをconcat\n",
    "\n",
    "train0 = train.copy() \n",
    "train = pd.concat([train,train_pl],axis=0,ignore_index=True)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3uASun3SF8G7"
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_LwpljEmMv3m"
   },
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    display(train.groupby('fold').size())\n",
    "    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n",
    "    display(train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZByzMMlLMv3m"
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# tokenizer\n",
    "# ====================================================\n",
    "\n",
    "# tokenizerに略語を追加していく\n",
    "abbreviations = ['h2o', 'conh2', 'vegfr2', 'her2']\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n",
    "tokenizer.add_tokens(abbreviations, special_tokens=False)\n",
    "tokenizer.save_pretrained(EXP_TOKENIZER)\n",
    "CFG.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1zqIZCneMv3n"
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Define max_len\n",
    "# ====================================================\n",
    "lengths_dict = {}\n",
    "\n",
    "lengths = []\n",
    "tk0 = tqdm(cpc_texts.values(), total=len(cpc_texts))\n",
    "for text in tk0:\n",
    "  length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "  lengths.append(length)\n",
    "lengths_dict['context_text'] = lengths\n",
    "\n",
    "for text_col in ['anchor', 'target']:\n",
    "    lengths = []\n",
    "    tk0 = tqdm(train[text_col].fillna(\"\").values, total=len(train))\n",
    "    for text in tk0:\n",
    "        length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "        lengths.append(length)\n",
    "    lengths_dict[text_col] = lengths\n",
    "    \n",
    "CFG.max_len = max(lengths_dict['anchor']) + max(lengths_dict['target'])\\\n",
    "                + max(lengths_dict['context_text']) + 4 # CLS + SEP + SEP + SEP\n",
    "logger.info(f\"model: {CFG.model}\")\n",
    "logger.info(f\"max_len: {CFG.max_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6HkKxDDMv3o"
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Dataset\n",
    "# ====================================================\n",
    "\n",
    "def prepare_input(cfg, text):\n",
    "  inputs = cfg.tokenizer(text,\n",
    "                         add_special_tokens=True,\n",
    "                         max_length=cfg.max_len,\n",
    "                         padding=\"max_length\",\n",
    "                         return_offsets_mapping=False)\n",
    "  for k, v in inputs.items():\n",
    "    inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "\n",
    "  return inputs\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "  def __init__(self, cfg, df):\n",
    "    self.cfg = cfg\n",
    "    self.texts = df[\"text\"].values\n",
    "    self.labels = df[\"score\"].values\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "\n",
    "  def __getitem__(self, item):\n",
    "    # item は何個とりだすかの設定\n",
    "    inputs = prepare_input(self.cfg, self.texts[item])\n",
    "    label = torch.tensor(self.labels[item], dtype=torch.float)\n",
    "    return inputs, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dMUQge-iPOTa"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qM3kSq0PMv3p"
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Model\n",
    "# ====================================================\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n",
    "            token_embeddings_size = len(CFG.tokenizer)\n",
    "            self.model.resize_token_embeddings(token_embeddings_size)\n",
    "        else:\n",
    "            self.model = AutoModel.from_config(self.config)\n",
    "\n",
    "        self.fc_dropout1 = nn.Dropout(0.1)\n",
    "        self.fc_dropout2 = nn.Dropout(0.2)\n",
    "        self.fc_dropout3 = nn.Dropout(0.3)\n",
    "        self.fc_dropout4 = nn.Dropout(0.4)\n",
    "        self.fc_dropout5 = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n",
    "        self._init_weights(self.fc)\n",
    "        # これがいわゆるattention pool\n",
    "        # https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching/discussion/324330\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(self.config.hidden_size, self.config.hidden_size),\n",
    "            nn.LayerNorm(self.config.hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.config.hidden_size, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self._init_weights(self.attention)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            #module.weight.data.normal_(mean=0.0,std=0.02) # ★★★ bigpatent用\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            #module.weight.data.normal_(mean=0.0, std=0.02) # ★★★ bigpatent用\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        \n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs[0]\n",
    "        # feature = torch.mean(last_hidden_states, 1)\n",
    "        weights = self.attention(last_hidden_states)\n",
    "        feature = torch.sum(weights * last_hidden_states, dim=1)\n",
    "        return feature\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature(inputs)\n",
    "\n",
    "        feature1 = self.fc_dropout1(feature)\n",
    "        feature2 = self.fc_dropout2(feature)\n",
    "        feature3 = self.fc_dropout3(feature)\n",
    "        feature4 = self.fc_dropout4(feature)\n",
    "        feature5 = self.fc_dropout5(feature)\n",
    "\n",
    "        feature_all = (feature1+feature2+feature3+feature4+feature5)/5\n",
    "        output = self.fc(feature_all)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "An3TQ61WPRnP"
   },
   "source": [
    "# Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99zfWbUTMv3p"
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Helper functions\n",
    "# ====================================================\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1uZ64YLjMv3q"
   },
   "outputs": [],
   "source": [
    "def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, (inputs, labels) in enumerate(train_loader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            y_preds = model(inputs)\n",
    "        loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            if CFG.batch_scheduler:\n",
    "                scheduler.step()\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.8f}  '\n",
    "                  .format(epoch+1, step, len(train_loader), \n",
    "                          remain=timeSince(start, float(step+1)/len(train_loader)),\n",
    "                          loss=losses,\n",
    "                          grad_norm=grad_norm,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "        if CFG.wandb:\n",
    "            wandb.log({f\"[fold{fold}] loss\": losses.val,\n",
    "                       f\"[fold{fold}] lr\": scheduler.get_lr()[0]})\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    for step, (inputs, labels) in enumerate(valid_loader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(y_preds.sigmoid().to('cpu').numpy())\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, len(valid_loader),\n",
    "                          loss=losses,\n",
    "                          remain=timeSince(start, float(step+1)/len(valid_loader))))\n",
    "    predictions = np.concatenate(preds)\n",
    "    predictions = np.concatenate(predictions)\n",
    "    return losses.avg, predictions\n",
    "\n",
    "\n",
    "def inference_fn(test_loader, model, device):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "    for inputs in tk0:\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        preds.append(y_preds.sigmoid().to('cpu').numpy())\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fv-WtjJIPGGm"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SICI8Z6CMv3s"
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# train loop\n",
    "# ====================================================\n",
    "\n",
    "def train_loop(folds,folds0, fold):\n",
    "  \"\"\"\n",
    "  folds: df\n",
    "  fold: fold\n",
    "  \"\"\"\n",
    "  logger.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "  # ====================================================\n",
    "  # loader\n",
    "  # ====================================================\n",
    "  train_folds = folds[folds['fold']!=fold].reset_index(drop=True)\n",
    "  valid_folds = folds0[folds0['fold']==fold].reset_index(drop=True)\n",
    "  valid_labels = valid_folds[\"score\"].values\n",
    "\n",
    "  train_dataset = TrainDataset(CFG, train_folds)\n",
    "  valid_dataset = TrainDataset(CFG, valid_folds)\n",
    "\n",
    "  train_loader = DataLoader(train_dataset,\n",
    "                            batch_size = CFG.batch_size,\n",
    "                            shuffle=True,\n",
    "                            num_workers=CFG.num_workers,\n",
    "                            pin_memory=True,\n",
    "                            drop_last=True\n",
    "                            )\n",
    "  valid_loader = DataLoader(valid_dataset,\n",
    "                            batch_size=CFG.batch_size,\n",
    "                            shuffle=False,\n",
    "                            num_workers=CFG.num_workers, \n",
    "                            pin_memory=True, \n",
    "                            drop_last=False\n",
    "                            )\n",
    "  # ====================================================\n",
    "  # model\n",
    "  # ====================================================\n",
    "  model = CustomModel(CFG, config_path=None, pretrained=True)\n",
    "  torch.save(model.config, os.path.join(OUTPUT_EXP, 'config.pth'))\n",
    "  model.to(device)\n",
    "\n",
    "  # ====================================================\n",
    "  # optimizer\n",
    "  # ====================================================\n",
    "\n",
    "  def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "          'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "          'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "          'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "    ]\n",
    "    return optimizer_parameters\n",
    "\n",
    "  optimizer_parameters = get_optimizer_params(model,\n",
    "                                                encoder_lr=CFG.encoder_lr, \n",
    "                                                decoder_lr=CFG.decoder_lr,\n",
    "                                                weight_decay=CFG.weight_decay)\n",
    "  optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)\n",
    "  #optimizer = bnb.optim.Adam8bit(optimizer_parameters, lr=CFG.encoder_lr, betas=CFG.betas)　★★★\n",
    "\n",
    "  # ====================================================\n",
    "  # scheduler\n",
    "  # ====================================================\n",
    "  def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "    if cfg.scheduler == 'linear':\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "        )\n",
    "    elif cfg.scheduler == 'cosine':\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n",
    "        )\n",
    "    elif cfg.scheduler=='ReduceLROnPlateau':\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=CFG.factor, patience=CFG.patience, verbose=True, eps=CFG.eps)\n",
    "    elif cfg.scheduler=='CosineAnnealingLR':\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr, last_epoch=-1)\n",
    "    elif cfg.scheduler=='CosineAnnealingWarmRestarts':\n",
    "        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=1, eta_min=CFG.min_lr, last_epoch=-1)\n",
    "    return scheduler\n",
    "\n",
    "  num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n",
    "  scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "\n",
    "  # ====================================================\n",
    "  # loop\n",
    "  # ====================================================\n",
    "  criterion = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "  best_score = 0.\n",
    "\n",
    "  for epoch in range(CFG.epochs):\n",
    "    start_time = time.time()\n",
    "    # train\n",
    "    avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n",
    "    # eval\n",
    "    avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device)\n",
    "    # scoring\n",
    "    score = get_score(valid_labels, predictions)\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    logger.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "    logger.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n",
    "\n",
    "    if CFG.wandb:\n",
    "      wandb.log({f\"[fold{fold}] epoch\": epoch+1, \n",
    "                 f\"[fold{fold}] avg_train_loss\": avg_loss, \n",
    "                 f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n",
    "                 f\"[fold{fold}] score\": score})\n",
    "      \n",
    "    if best_score < score:\n",
    "      best_score = score\n",
    "      logger.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "      torch.save({'model': model.state_dict(),\n",
    "                  'predictions': predictions},\n",
    "                 os.path.join(EXP_MODEL,f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\"))\n",
    "\n",
    "  predictions = torch.load(os.path.join(EXP_MODEL,f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\"), \n",
    "                             map_location=torch.device('cpu'))['predictions']\n",
    "  valid_folds['pred'] = predictions\n",
    "\n",
    "  torch.cuda.empty_cache()\n",
    "  gc.collect()\n",
    "    \n",
    "  return valid_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqyeIeNqPJ3L"
   },
   "source": [
    "#Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vv9pTbCDMJCy"
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    def get_result(oof_df):\n",
    "        labels = oof_df['score'].values\n",
    "        preds = oof_df['pred'].values\n",
    "        score = get_score(labels, preds)\n",
    "        logger.info(f'Score: {score:<.4f}')\n",
    "    \n",
    "    if CFG.train:\n",
    "        oof_df = pd.DataFrame()\n",
    "        for fold in range(CFG.n_fold):\n",
    "            if fold in CFG.trn_fold:\n",
    "                _oof_df = train_loop(train,train0, fold) # ★★★変更点\n",
    "                oof_df = pd.concat([oof_df, _oof_df])\n",
    "                logger.info(f\"========== fold: {fold} result ==========\")\n",
    "                get_result(_oof_df)\n",
    "        oof_df = oof_df.reset_index(drop=True)\n",
    "        logger.info(f\"========== CV ==========\")\n",
    "        get_result(oof_df)\n",
    "        oof_df.to_pickle(os.path.join(EXP_PREDS, 'oof_df.pkl'))\n",
    "        \n",
    "    if CFG.wandb:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rj3DCps3MJBT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9F-AuGVTmG-u"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sHnczi2JQUvK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "izg37NiWhVrq"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prulv4kjMv3u"
   },
   "outputs": [],
   "source": [
    "# upload output folder to kaggle dataset\n",
    "if CFG.upload_from_colab:\n",
    "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "    def dataset_create_new(dataset_name, upload_dir):\n",
    "        dataset_metadata = {}\n",
    "        dataset_metadata['id'] = f'{os.environ[\"KAGGLE_USERNAME\"]}/{dataset_name}'\n",
    "        dataset_metadata['licenses'] = [{'name': 'CC0-1.0'}]\n",
    "        dataset_metadata['title'] = dataset_name\n",
    "        with open(os.path.join(upload_dir, 'dataset-metadata.json'), 'w') as f:\n",
    "            json.dump(dataset_metadata, f, indent=4)\n",
    "        api = KaggleApi()\n",
    "        api.authenticate()\n",
    "        api.dataset_create_new(folder=upload_dir, convert_to_csv=False, dir_mode='tar')\n",
    "\n",
    "    dataset_create_new(dataset_name=CFG.competition + \"-\" + CFG.name, upload_dir=OUTPUT_EXP)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "kfsky-baseline029_PL.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
