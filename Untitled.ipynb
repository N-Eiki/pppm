{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc37669d-f87b-48f1-89e1-596ed91f5f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /usr/share/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# !pip install nltk\n",
    "from nltk import stem\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29a553ea-6360-45bc-a76c-5a04917b3171",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_260/2453705756.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# print(stemmer.stem(s1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/us-patent-phrase-to-phrase-matching/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "stemmer = stem.PorterStemmer()\n",
    "# print(stemmer.stem(s1))\n",
    "\n",
    "train = pd.read_csv('../data/us-patent-phrase-to-phrase-matching/train.csv')\n",
    "train = pd.concat([train])\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0853f7a-f535-49ea-9791-22c2f08d05dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for col in [\"anchor\", \"target\"]:\n",
    "    train[f\"{col}_stem\"] = train[col].apply(lambda words: [stemmer.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8007a5a-6018-4381-83bb-cda7775e8842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install swifter\n",
    "# import swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693c67f9-5e03-4316-96ba-b716ec78331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# for col in [\"anchor\", \"target\"]:\n",
    "#     train[f\"{col}_stem\"] = train[col].swifter.apply(lambda words :[stemmer.stem(word) for word in words.split(\" \")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a559df-2cc9-4382-9f6b-32aea0ea504f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for col in [\"anchor\", \"target\"]:\n",
    "    stems = []\n",
    "    for words in train[col].values:\n",
    "        stems.append([stemmer.stem(word) for word in words.split(\" \")])\n",
    "    train[f\"{col}_stem\"] = stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4bb7e0-7f71-47cd-84b6-5fd0e2d10c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# def stemming(words):\n",
    "#     return [stemmer.stem(word) for word  in words.split(\" \")]\n",
    "\n",
    "# for col in [\"anchor\", \"target\"]:\n",
    "#     train[f\"{col}_stem\"] = np.vectorize(stemming)(train[col].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82592754-f7fc-4500-8e01-4effb03d225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[\"same_stem_num\"] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b702a6-35d1-4d04-9e64-e9f3a27c0e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_stems = []\n",
    "same_stems_num = []\n",
    "for anchor, target in zip(train.anchor_stem.values, train.target_stem.values):\n",
    "    same_stem = list(set(anchor)&set(target))\n",
    "    same_stems.append(same_stem)\n",
    "    same_stems_num.append(len(same_stem))\n",
    "    \n",
    "train[\"same_stem\"] = same_stems\n",
    "train[\"same_stem_num\"] = same_stems_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d626d80-4542-4bbe-9e2d-5f01a5abae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train[train.score>=0.75].same_stem_num.mean(), train[train.score>=0.75].shape)\n",
    "# print(train[train.score>=0.5].same_stem_num.mean())\n",
    "# print(train[train.score>=0.25].same_stem_num.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f4755e-7c0a-4e70-bf39-ad4afae576d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train[train.score>=0.75].same_stem_num.mean(), train[train.score>=0.75].shape)\n",
    "# print(train[train.score>=0.5][train.score<0.75].same_stem_num.mean(), train[train.score>=0.5][train.score<0.75].shape)\n",
    "# print(train[train.score>=0.25][train.score<0.5].same_stem_num.mean(), train[train.score>=0.25][train.score<0.5].shape)\n",
    "# print(train[train.score<0.25].same_stem_num.mean(), train[train.score<0.25].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf81589-94c8-46f9-9ade-e4c1a999cfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[train.score>=0.75]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0edbbf-c372-4865-a08a-cf3670816c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hoge = train[train.score<0.25]\n",
    "# hoge[hoge.same_stem_num>0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a012e328-9f6c-4290-b391-86f4474127dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk import word_tokenize\n",
    "# from nltk import bleu_score\n",
    "# nltk.download('punkt')\n",
    "# hy = \"act of abating\"\n",
    "# hy = word_tokenize(hy)\n",
    "# re = \"abatement\"\n",
    "# re = word_tokenize(re)\n",
    "# res = [ re ]\n",
    "\n",
    "# BLEUscore = bleu_score.sentence_bleu(res, hy)\n",
    "# print(\"score : \" + str( BLEUscore ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdb9bb2-6df9-4b32-bd13-d77c6539d3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1= \"abatement\"\n",
    "text2 = \"act of abating\"\n",
    "# print(nltk.pos_tag(text1.split(' ')))\n",
    "# print(nltk.pos_tag(text2.split(' ')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411ba2a5-401b-4daf-b208-484ee91a2e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim\n",
    "# import gensim\n",
    "# word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('model/model_neologd.vec', binary=False)\n",
    "\n",
    "# def avg_feature_vector(sentence, model, num_features):\n",
    "#     words = mecab.parse(sentence).replace(' \\n', '').split() # mecabの分かち書きでは最後に改行(\\n)が出力されてしまうため、除去\n",
    "#     feature_vec = np.zeros((num_features,), dtype=\"float32\") # 特徴ベクトルの入れ物を初期化\n",
    "#     for word in words:\n",
    "#         feature_vec = np.add(feature_vec, model[word])\n",
    "#     if len(words) > 0:\n",
    "#         feature_vec = np.divide(feature_vec, len(words))\n",
    "#     return feature_vec\n",
    "\n",
    "# def sentence_similarity(sentence_1, sentence_2):\n",
    "#     # 今回使うWord2Vecのモデルは300次元の特徴ベクトルで生成されているので、num_featuresも300に指定\n",
    "#     num_features=300\n",
    "#     sentence_1_avg_vector = avg_feature_vector(sentence_1, word2vec_model, num_features)\n",
    "#     sentence_2_avg_vector = avg_feature_vector(sentence_2, word2vec_model, num_features)\n",
    "#     # １からベクトル間の距離を引いてあげることで、コサイン類似度を計算\n",
    "#     return 1 - spatial.distance.cosine(sentence_1_avg_vector, sentence_2_avg_vector)\n",
    "\n",
    "# sentence_similarity(text1, text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2e995c-c848-4954-988d-09a3e7ca462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install bert_score\n",
    "# from bert_score import score\n",
    "\n",
    "# def calc_bert_score(cands, refs):\n",
    "#     \"\"\" BERTスコアの算出\n",
    "#     Args:\n",
    "#         cands ([List[str]]): [比較元の文]\n",
    "#         refs ([List[str]]): [比較対象の文]\n",
    "#     Returns:\n",
    "#         [(List[float], List[float], List[float])]: [(Precision, Recall, F1スコア)]\n",
    "#     \"\"\"\n",
    "#     Precision, Recall, F1 = score(cands, refs, lang=\"ja\", verbose=True)\n",
    "#     return Precision.numpy().tolist(), Recall.numpy().tolist(), F1.numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8771b0a9-a79d-45b3-8b08-587317ed102e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
