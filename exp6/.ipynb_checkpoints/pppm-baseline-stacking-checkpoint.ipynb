{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Directory settings\nfunnel-transformer-largeを追加\n\nhttps://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching/discussion/327288","metadata":{"papermill":{"duration":0.024515,"end_time":"2022-03-22T09:40:01.460332","exception":false,"start_time":"2022-03-22T09:40:01.435817","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Directory settings\n# ====================================================\nimport os\n\nINPUT_DIR = '../input/us-patent-phrase-to-phrase-matching/'\nOUTPUT_DIR = './'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)","metadata":{"id":"fa3b873b","papermill":{"duration":0.041313,"end_time":"2022-03-22T09:40:01.526545","exception":false,"start_time":"2022-03-22T09:40:01.485232","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-07T16:03:18.970763Z","iopub.execute_input":"2022-06-07T16:03:18.971545Z","iopub.status.idle":"2022-06-07T16:03:19.004296Z","shell.execute_reply.started":"2022-06-07T16:03:18.971418Z","shell.execute_reply":"2022-06-07T16:03:19.003249Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# CFG","metadata":{"id":"1d0c4430","papermill":{"duration":0.024609,"end_time":"2022-03-22T09:40:01.576366","exception":false,"start_time":"2022-03-22T09:40:01.551757","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# CFG\n# ====================================================\nclass CFG_exp016:\n    num_workers=4\n    path=\"../input/pppm-baseline-016/\"\n    config_path=path+'config.pth'\n    model=\"microsoft/deberta-v3-large\"\n    batch_size=32\n    fc_dropout=0.2\n    target_size=1\n    max_len=133\n    seed=42\n    n_fold=5\n    trn_fold=[0, 1, 2, 3, 4]\n\nclass CFG_exp026:\n    num_workers=4\n    path=\"../input/pppm-baseline-026/\"\n    config_path=path+'config.pth'\n    model=\"microsoft/deberta-v3-large\"\n    batch_size=32\n    fc_dropout=0.2\n    target_size=1\n    max_len=134\n    seed=42\n    n_fold=5\n    trn_fold=[0, 1, 2, 3, 4]\n    \nclass CFG_exp020:\n    num_workers=4\n    path=\"../input/pppm-baseline-020/\"\n    config_path=path+'config.pth'\n    model=\"roberta-large\"\n    batch_size=32\n    fc_dropout=0.2\n    target_size=1\n    max_len=175\n    seed=42\n    n_fold=5\n    trn_fold=[0, 1, 2, 3, 4]\n\n    \nclass CFG_exp011:\n    num_workers=4\n    path=\"../input/pppm-baseline-011/\"\n    config_path=path+'config.pth'\n    model=\"microsoft/deberta-v3-base\"\n    batch_size=32\n    fc_dropout=0.2\n    target_size=1\n    max_len=133\n    seed=42\n    n_fold=5\n    trn_fold=[0, 1, 2, 3, 4]\n\nclass CFG_exp018:\n    num_workers=4\n    path=\"../input/pppm-baseline-018/\"\n    config_path=path+'config.pth'\n    model=\"anferico/bert-for-patents\"\n    batch_size=32\n    fc_dropout=0.2\n    target_size=1\n    max_len=117\n    seed=42\n    n_fold=5\n    trn_fold=[0, 1, 2, 3, 4]\n    \n\nclass CFG_exp032:\n    num_workers=4\n    path=\"../input/pppm-baseline-032/\"\n    config_path=path+'config.pth'\n    model=\"funnel-transformer/large\"\n    batch_size=32\n    fc_dropout=0.2\n    target_size=1\n    max_len=125\n    seed=42\n    n_fold=5\n    trn_fold=[0, 1, 2, 3, 4]","metadata":{"id":"48dd82bb","papermill":{"duration":0.033949,"end_time":"2022-03-22T09:40:01.634977","exception":false,"start_time":"2022-03-22T09:40:01.601028","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-07T16:03:19.006302Z","iopub.execute_input":"2022-06-07T16:03:19.006760Z","iopub.status.idle":"2022-06-07T16:03:19.022540Z","shell.execute_reply.started":"2022-06-07T16:03:19.006724Z","shell.execute_reply":"2022-06-07T16:03:19.021679Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Library","metadata":{"id":"f2ed8ef2","papermill":{"duration":0.038261,"end_time":"2022-03-22T09:40:10.626926","exception":false,"start_time":"2022-03-22T09:40:10.588665","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Library\n# ====================================================\nimport os\nimport gc\nimport re\nimport ast\nimport sys\nimport copy\nimport json\nimport time\nimport math\nimport shutil\nimport string\nimport pickle\nimport random\nimport joblib\nimport itertools\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n\nimport torch\nprint(f\"torch.__version__: {torch.__version__}\")\nimport torch.nn as nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\n\nos.system('pip uninstall -y transformers')\nos.system('pip uninstall -y tokenizers')\nos.system('pip uninstall -y pytorch-tabnet')\n\nos.system('python -m pip install --no-index --find-links=../input/pppm-pip-wheels-dataset transformers')\nos.system('python -m pip install --no-index --find-links=../input/pppm-pip-wheels-dataset tokenizers')\nos.system('python -m pip install --no-index --find-links=../input/pytorchtabnet/pytorch-tabnet  pytorch-tabnet')\n\nimport tokenizers\nimport transformers\nprint(f\"tokenizers.__version__: {tokenizers.__version__}\")\nprint(f\"transformers.__version__: {transformers.__version__}\")\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n%env TOKENIZERS_PARALLELISM=true\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"executionInfo":{"elapsed":20123,"status":"ok","timestamp":1644920080956,"user":{"displayName":"Yasufumi Nakama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17486303986134302670"},"user_tz":-540},"id":"35916341","outputId":"06fa0ab8-a380-4f54-a98d-b7015b79d9e2","papermill":{"duration":26.143536,"end_time":"2022-03-22T09:40:36.798853","exception":false,"start_time":"2022-03-22T09:40:10.655317","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-07T16:03:19.024263Z","iopub.execute_input":"2022-06-07T16:03:19.024563Z","iopub.status.idle":"2022-06-07T16:04:08.804336Z","shell.execute_reply.started":"2022-06-07T16:03:19.024533Z","shell.execute_reply":"2022-06-07T16:04:08.803124Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{"id":"fd586614","papermill":{"duration":0.032888,"end_time":"2022-03-22T09:40:36.865209","exception":false,"start_time":"2022-03-22T09:40:36.832321","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Utils\n# ====================================================\ndef get_score(y_true, y_pred):\n    score = sp.stats.pearsonr(y_true, y_pred)[0]\n    return score\n\n\ndef get_logger(filename=OUTPUT_DIR+'train'):\n    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = get_logger()\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","metadata":{"id":"d5c0ccc6","papermill":{"duration":0.21551,"end_time":"2022-03-22T09:40:37.116848","exception":false,"start_time":"2022-03-22T09:40:36.901338","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-07T16:04:08.806339Z","iopub.execute_input":"2022-06-07T16:04:08.806587Z","iopub.status.idle":"2022-06-07T16:04:08.822223Z","shell.execute_reply.started":"2022-06-07T16:04:08.806556Z","shell.execute_reply":"2022-06-07T16:04:08.821002Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# OOF","metadata":{}},{"cell_type":"code","source":"## 別ver\n# https://www.kaggle.com/code/jellyz9/tips-for-ensambling\nfrom sklearn.preprocessing import MinMaxScaler #Scaler\nfrom sklearn.linear_model import LinearRegression #重回帰分析\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set()\n\nuse_models =[\n    \"../input/pppm-baseline-026\",\n    \"../input/pppm-baseline-020\",\n    \"../input/pppm-baseline-018\",\n    \"../input/pppm-baseline-011\",\n    \"../input/pppm-baseline-032\"\n]\n\ntrain_df = pd.read_csv(\"../input/us-patent-phrase-to-phrase-matching/train.csv\")\nens_df = train_df[[\"id\", \"score\"]]\n\nfor model in use_models:\n  _df = pd.read_pickle(os.path.join(model, \"preds\", \"oof_df.pkl\"))\n  if len(_df) > len(ens_df):\n    print(model)\n    _df = _df[_df[\"flag\"]==1]\n    df = _df[[\"id\", \"pred\"]]\n    \n    MMscaler = MinMaxScaler()\n    df[\"pred\"] = MMscaler.fit_transform(df[\"pred\"].values.reshape(-1,1)).reshape(-1)\n\n    df.rename(columns={\"pred\":model.split(\"/\")[-1]},inplace=True)\n    ens_df = pd.merge(ens_df, df, on=\"id\", how=\"left\")\n\n  else:\n    df = _df[[\"id\", \"pred\"]]\n\n    MMscaler = MinMaxScaler()\n    df[\"pred\"] = MMscaler.fit_transform(df[\"pred\"].values.reshape(-1,1)).reshape(-1)\n\n    df.rename(columns={\"pred\":model.split(\"/\")[-1]},inplace=True)\n    ens_df = pd.merge(ens_df, df, on=\"id\", how=\"left\")\n\nens_df\n","metadata":{"execution":{"iopub.status.busy":"2022-06-07T16:04:08.823945Z","iopub.execute_input":"2022-06-07T16:04:08.824706Z","iopub.status.idle":"2022-06-07T16:04:10.095514Z","shell.execute_reply.started":"2022-06-07T16:04:08.824658Z","shell.execute_reply":"2022-06-07T16:04:10.094011Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# アンサンブル\n\nY = ens_df[\"score\"]\nX = ens_df.drop([\"score\",\"id\"], axis=1)\n\nplt.figure(figsize=(8,8))\ncolormap = plt.cm.RdBu\nsns.heatmap(ens_df.drop(\"id\",axis=1).astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)\n\n# mdoel\nmodels = []\nens_scores = []\nva_idxes = []\npreds = []\n\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nfor i, (tr_idx, val_idx) in enumerate(list(kf.split(X, Y))):\n    tr_x, tr_y = X.values[tr_idx], Y.values[tr_idx]\n    val_x, val_y = X.values[val_idx], Y.values[val_idx]\n    \n    model = LinearRegression()\n    model.fit(tr_x, tr_y)\n    pred = model.predict(val_x)\n    score = get_score(pred, val_y)\n    \n    \n    models.append(model)\n    ens_scores.append(score)\n    va_idxes.append(val_idx)\n    preds.append(pred)\n\nva_idxes = np.concatenate(va_idxes)\norder = np.argsort(va_idxes)\npreds = np.concatenate(preds, axis=0)\npreds = preds[order]\n\nfinal_score = get_score(Y, preds)\nprint(final_score)\nLOGGER.info(f'CV Score: {final_score:<.4f}')","metadata":{"execution":{"iopub.status.busy":"2022-06-07T16:04:10.097235Z","iopub.execute_input":"2022-06-07T16:04:10.098238Z","iopub.status.idle":"2022-06-07T16:04:10.859222Z","shell.execute_reply.started":"2022-06-07T16:04:10.098176Z","shell.execute_reply":"2022-06-07T16:04:10.858117Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# stacking oof","metadata":{}},{"cell_type":"code","source":"stacking_dir = \"../input/5model-linear-stacking/5model_kfsky_linear\"\ndirs = os.listdir(stacking_dir)\nens_df = []\n\nfor dir in dirs:\n    path = os.path.join(stacking_dir, dir, \"oof_df.csv\")\n    print(path)\n    df = pd.read_csv(path)\n    ens_df.append(df[\"score\"].values)\n    \nens_df = pd.DataFrame(ens_df).T\nens_df[[\"id\", \"score\"]] =  train_df[[\"id\", \"score\"]]\nprint(ens_df.shape)\nens_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-07T16:04:10.861405Z","iopub.execute_input":"2022-06-07T16:04:10.862790Z","iopub.status.idle":"2022-06-07T16:04:13.237433Z","shell.execute_reply.started":"2022-06-07T16:04:10.862730Z","shell.execute_reply":"2022-06-07T16:04:13.236096Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# アンサンブル\n\n# Y = ens_df[\"score\"]\nX = ens_df.drop([\"score\",\"id\"], axis=1)\n\n# plt.figure(figsize=(8,8))\n# colormap = plt.cm.RdBu\n# sns.heatmap(ens_df.drop(\"id\",axis=1).astype(float).corr(),linewidths=0.1,vmax=1.0, \n#             square=True, cmap=colormap, linecolor='white', annot=True)\n\n# mdoel\nmodels = []\nens_scores = []\nva_idxes = []\npreds = []\n\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nfor i, (tr_idx, val_idx) in enumerate(list(kf.split(X, Y))):\n    tr_x, tr_y = X.values[tr_idx], Y.values[tr_idx]\n    val_x, val_y = X.values[val_idx], Y.values[val_idx]\n    \n    model = LinearRegression()\n    model.fit(tr_x, tr_y)\n    pred = model.predict(val_x)\n    score = get_score(pred, val_y)\n    \n    \n    models.append(model)\n    ens_scores.append(score)\n    va_idxes.append(val_idx)\n    preds.append(pred)\n\nva_idxes = np.concatenate(va_idxes)\norder = np.argsort(va_idxes)\npreds = np.concatenate(preds, axis=0)\npreds = preds[order]\n\nfinal_score = get_score(Y, preds)\nprint(final_score)\nLOGGER.info(f'CV Score: {final_score:<.4f}')","metadata":{"execution":{"iopub.status.busy":"2022-06-07T16:04:13.238536Z","iopub.execute_input":"2022-06-07T16:04:13.238772Z","iopub.status.idle":"2022-06-07T16:04:13.337311Z","shell.execute_reply.started":"2022-06-07T16:04:13.238744Z","shell.execute_reply":"2022-06-07T16:04:13.335738Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Data Loading","metadata":{"id":"cb3d8e1e","papermill":{"duration":0.032614,"end_time":"2022-03-22T09:40:37.184739","exception":false,"start_time":"2022-03-22T09:40:37.152125","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Data Loading\n# ====================================================\ntest = pd.read_csv(INPUT_DIR+'test.csv')\nsubmission = pd.read_csv(INPUT_DIR+'sample_submission.csv')\nprint(f\"test.shape: {test.shape}\")\nprint(f\"submission.shape: {submission.shape}\")\ndisplay(test.head())\ndisplay(submission.head())","metadata":{"executionInfo":{"elapsed":2627,"status":"ok","timestamp":1644920084001,"user":{"displayName":"Yasufumi Nakama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17486303986134302670"},"user_tz":-540},"id":"bef012d3","outputId":"d4d60dbc-510c-4f34-8d64-dd1d88c4808c","papermill":{"duration":0.154829,"end_time":"2022-03-22T09:40:37.374453","exception":false,"start_time":"2022-03-22T09:40:37.219624","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-07T16:04:13.340162Z","iopub.execute_input":"2022-06-07T16:04:13.341609Z","iopub.status.idle":"2022-06-07T16:04:13.405790Z","shell.execute_reply.started":"2022-06-07T16:04:13.341538Z","shell.execute_reply":"2022-06-07T16:04:13.404622Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# CPC Data\n# ====================================================\n\ndef omit_char(x):\n  x = x.replace(\";\", \"\") # ; を削除\n  x = x.replace(\"[\", \"\") # [] を削除\n  x = x.replace(\"]\", \"\") # [] を削除\n  x = x.lower() # すべて小文字に変換\n  return x\n\ncpc_texts = torch.load(os.path.join(CFG_exp016.path, \"cpc_texts.pth\"))\ntest['context_text'] = test['context'].map(cpc_texts)\ntest['anchor'].replace(\"dry coating composition1\", \"dry coating composition\", inplace=True)\n\n\ntest['context_text'] = test['context_text'].map(omit_char)\ntest['context_text'] = test['context_text'].replace(\"human necessities. griculture forestry animal husbandry hunting trapping fishing\", \n                                                      \"human necessities. agriculture forestry animal husbandry hunting trapping fishing\")\ntest['context_text'] = test['context_text'].str.replace(\"hemistry\", \"chemistry\")\ndisplay(test.head())","metadata":{"papermill":{"duration":0.848818,"end_time":"2022-03-22T09:40:38.260255","exception":false,"start_time":"2022-03-22T09:40:37.411437","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-07T16:04:13.417892Z","iopub.execute_input":"2022-06-07T16:04:13.421118Z","iopub.status.idle":"2022-06-07T16:04:13.461773Z","shell.execute_reply.started":"2022-06-07T16:04:13.421050Z","shell.execute_reply":"2022-06-07T16:04:13.460937Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"test['text'] = test['anchor'] + '[SEP]' + test['target'] + '[SEP]'  + test['context_text']\ndisplay(test.head())","metadata":{"papermill":{"duration":0.084831,"end_time":"2022-03-22T09:40:38.384239","exception":false,"start_time":"2022-03-22T09:40:38.299408","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-07T16:04:13.463371Z","iopub.execute_input":"2022-06-07T16:04:13.463814Z","iopub.status.idle":"2022-06-07T16:04:13.599240Z","shell.execute_reply.started":"2022-06-07T16:04:13.463781Z","shell.execute_reply":"2022-06-07T16:04:13.598087Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# exp026","metadata":{}},{"cell_type":"markdown","source":"## tokenizer","metadata":{"id":"918a28aa","papermill":{"duration":0.039494,"end_time":"2022-03-22T09:40:39.374931","exception":false,"start_time":"2022-03-22T09:40:39.335437","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# tokenizer\n# ====================================================\nCFG_exp026.tokenizer = AutoTokenizer.from_pretrained(CFG_exp026.path+'tokenizer/')","metadata":{"papermill":{"duration":5.198604,"end_time":"2022-03-22T09:40:44.612849","exception":false,"start_time":"2022-03-22T09:40:39.414245","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-07T16:04:13.602176Z","iopub.execute_input":"2022-06-07T16:04:13.602884Z","iopub.status.idle":"2022-06-07T16:04:14.711167Z","shell.execute_reply.started":"2022-06-07T16:04:13.602842Z","shell.execute_reply":"2022-06-07T16:04:14.709496Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{"id":"14da40cf","papermill":{"duration":0.04897,"end_time":"2022-03-22T09:40:44.706931","exception":false,"start_time":"2022-03-22T09:40:44.657961","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Dataset\n# ====================================================\ndef prepare_input(cfg, text):\n    inputs = cfg.tokenizer(text,\n                           add_special_tokens=True,\n                           max_length=cfg.max_len,\n                           padding=\"max_length\",\n                           return_offsets_mapping=False)\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.texts = df['text'].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, self.texts[item])\n        return inputs","metadata":{"id":"9f791a19","papermill":{"duration":0.055528,"end_time":"2022-03-22T09:40:52.072178","exception":false,"start_time":"2022-03-22T09:40:52.01665","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-07T16:04:14.713023Z","iopub.execute_input":"2022-06-07T16:04:14.714180Z","iopub.status.idle":"2022-06-07T16:04:14.727868Z","shell.execute_reply.started":"2022-06-07T16:04:14.714100Z","shell.execute_reply":"2022-06-07T16:04:14.726487Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{"id":"e04d6363","papermill":{"duration":0.044161,"end_time":"2022-03-22T09:40:52.262022","exception":false,"start_time":"2022-03-22T09:40:52.217861","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ====================================================\n# Model\n# ====================================================\nclass CustomModel_exp026(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n        self._init_weights(self.fc)\n        self.attention = nn.Sequential(\n            nn.Linear(self.config.hidden_size, self.config.hidden_size),\n            nn.LayerNorm(self.config.hidden_size),\n            nn.GELU(),\n            nn.Linear(self.config.hidden_size, 1),\n            nn.Softmax(dim=1)\n        )\n        self.fc_dropout1 = nn.Dropout(0.1)\n        self.fc_dropout2 = nn.Dropout(0.2)\n        self.fc_dropout3 = nn.Dropout(0.3)\n        self.fc_dropout4 = nn.Dropout(0.4)\n        self.fc_dropout5 = nn.Dropout(0.5)\n        \n        self._init_weights(self.attention)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        # feature = torch.mean(last_hidden_states, 1)\n        weights = self.attention(last_hidden_states)\n        feature = torch.sum(weights * last_hidden_states, dim=1)\n        return feature\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n\n        feature1 = self.fc_dropout1(feature)\n        feature2 = self.fc_dropout2(feature)\n        feature3 = self.fc_dropout3(feature)\n        feature4 = self.fc_dropout4(feature)\n        feature5 = self.fc_dropout5(feature)\n\n        feature_all = (feature1+feature2+feature3+feature4+feature5)/5\n        output = self.fc(feature_all)\n        \n        return output","metadata":{"id":"4c5bab44","papermill":{"duration":0.066203,"end_time":"2022-03-22T09:40:52.37203","exception":false,"start_time":"2022-03-22T09:40:52.305827","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-07T16:04:14.731725Z","iopub.execute_input":"2022-06-07T16:04:14.732836Z","iopub.status.idle":"2022-06-07T16:04:14.808496Z","shell.execute_reply.started":"2022-06-07T16:04:14.732779Z","shell.execute_reply":"2022-06-07T16:04:14.807719Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# inference\n# ====================================================\ndef inference_fn(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    for inputs in tk0:\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        preds.append(y_preds.sigmoid().to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2022-06-07T16:04:14.810250Z","iopub.execute_input":"2022-06-07T16:04:14.810902Z","iopub.status.idle":"2022-06-07T16:04:14.832348Z","shell.execute_reply.started":"2022-06-07T16:04:14.810854Z","shell.execute_reply":"2022-06-07T16:04:14.830883Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(CFG_exp026, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG_exp026.batch_size,\n                         shuffle=False,\n                         num_workers=CFG_exp026.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in CFG_exp026.trn_fold:\n    model = CustomModel_exp026(CFG_exp026, config_path=CFG_exp026.config_path, pretrained=False)\n    state = torch.load(os.path.join(CFG_exp026.path,\"model\" , f\"{CFG_exp026.model.replace('/', '-')}_fold{fold}_best.pth\"),\n                       map_location=torch.device('cpu'))\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    predictions.append(prediction)\n    del model, state, prediction; gc.collect()\n    torch.cuda.empty_cache()\npredictions_exp026 = np.mean(predictions, axis=0)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-07T16:04:14.834292Z","iopub.execute_input":"2022-06-07T16:04:14.834919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# exp020","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# tokenizer\n# ====================================================\nCFG_exp020.tokenizer = AutoTokenizer.from_pretrained(CFG_exp020.path+'tokenizer/')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Model\n# ====================================================\nclass CustomModel_exp020(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n        self._init_weights(self.fc)\n        self.attention = nn.Sequential(\n            nn.Linear(self.config.hidden_size, self.config.hidden_size),\n            nn.LayerNorm(self.config.hidden_size),\n            nn.GELU(),\n            nn.Linear(self.config.hidden_size, 1),\n            nn.Softmax(dim=1)\n        )\n        self.fc_dropout1 = nn.Dropout(0.1)\n        self.fc_dropout2 = nn.Dropout(0.2)\n        self.fc_dropout3 = nn.Dropout(0.3)\n        self.fc_dropout4 = nn.Dropout(0.4)\n        self.fc_dropout5 = nn.Dropout(0.5)\n        \n        self._init_weights(self.attention)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        # feature = torch.mean(last_hidden_states, 1)\n        weights = self.attention(last_hidden_states)\n        feature = torch.sum(weights * last_hidden_states, dim=1)\n        return feature\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n\n        feature1 = self.fc_dropout1(feature)\n        feature2 = self.fc_dropout2(feature)\n        feature3 = self.fc_dropout3(feature)\n        feature4 = self.fc_dropout4(feature)\n        feature5 = self.fc_dropout5(feature)\n\n        feature_all = (feature1+feature2+feature3+feature4+feature5)/5\n        output = self.fc(feature_all)\n        \n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(CFG_exp020, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG_exp020.batch_size,\n                         shuffle=False,\n                         num_workers=CFG_exp020.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in CFG_exp020.trn_fold:\n    model = CustomModel_exp020(CFG_exp020, config_path=CFG_exp020.config_path, pretrained=False)\n    state = torch.load(os.path.join(CFG_exp020.path,\"model\" , f\"{CFG_exp020.model.replace('/', '-')}_fold{fold}_best.pth\"),\n                       map_location=torch.device('cpu'))\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    predictions.append(prediction)\n    del model, state, prediction; gc.collect()\n    torch.cuda.empty_cache()\npredictions_exp020 = np.mean(predictions, axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# exp011","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# tokenizer\n# ====================================================\nCFG_exp011.tokenizer = AutoTokenizer.from_pretrained(CFG_exp011.path+'tokenizer/')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Model\n# ====================================================\nclass CustomModel_exp011(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n        self._init_weights(self.fc)\n        self.attention = nn.Sequential(\n            nn.Linear(self.config.hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n        self.fc_dropout1 = nn.Dropout(0.1)\n        self.fc_dropout2 = nn.Dropout(0.2)\n        self.fc_dropout3 = nn.Dropout(0.3)\n        self.fc_dropout4 = nn.Dropout(0.4)\n        self.fc_dropout5 = nn.Dropout(0.5)\n        \n        self._init_weights(self.attention)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        # feature = torch.mean(last_hidden_states, 1)\n        weights = self.attention(last_hidden_states)\n        feature = torch.sum(weights * last_hidden_states, dim=1)\n        return feature\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n\n        feature1 = self.fc_dropout1(feature)\n        feature2 = self.fc_dropout2(feature)\n        feature3 = self.fc_dropout3(feature)\n        feature4 = self.fc_dropout4(feature)\n        feature5 = self.fc_dropout5(feature)\n\n        feature_all = (feature1+feature2+feature3+feature4+feature5)/5\n        output = self.fc(feature_all)\n        \n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(CFG_exp011, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG_exp011.batch_size,\n                         shuffle=False,\n                         num_workers=CFG_exp011.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in CFG_exp011.trn_fold:\n    model = CustomModel_exp011(CFG_exp011, config_path=CFG_exp011.config_path, pretrained=False)\n    state = torch.load(os.path.join(CFG_exp011.path,\"model\" , f\"{CFG_exp011.model.replace('/', '-')}_fold{fold}_best.pth\"),\n                       map_location=torch.device('cpu'))\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    predictions.append(prediction)\n    del model, state, prediction; gc.collect()\n    torch.cuda.empty_cache()\npredictions_exp011 = np.mean(predictions, axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# exp018","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# tokenizer\n# ====================================================\nCFG_exp018.tokenizer = AutoTokenizer.from_pretrained(CFG_exp018.path+'tokenizer/')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Model\n# ====================================================\nclass CustomModel_exp018(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n        self._init_weights(self.fc)\n        self.attention = nn.Sequential(\n            nn.Linear(self.config.hidden_size, self.config.hidden_size),\n            nn.LayerNorm(self.config.hidden_size),\n            nn.GELU(),\n            nn.Linear(self.config.hidden_size, 1),\n            nn.Softmax(dim=1)\n        )\n        self.fc_dropout1 = nn.Dropout(0.1)\n        self.fc_dropout2 = nn.Dropout(0.2)\n        self.fc_dropout3 = nn.Dropout(0.3)\n        self.fc_dropout4 = nn.Dropout(0.4)\n        self.fc_dropout5 = nn.Dropout(0.5)\n        \n        self._init_weights(self.attention)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        # feature = torch.mean(last_hidden_states, 1)\n        weights = self.attention(last_hidden_states)\n        feature = torch.sum(weights * last_hidden_states, dim=1)\n        return feature\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n\n        feature1 = self.fc_dropout1(feature)\n        feature2 = self.fc_dropout2(feature)\n        feature3 = self.fc_dropout3(feature)\n        feature4 = self.fc_dropout4(feature)\n        feature5 = self.fc_dropout5(feature)\n\n        feature_all = (feature1+feature2+feature3+feature4+feature5)/5\n        output = self.fc(feature_all)\n        \n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(CFG_exp018, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG_exp018.batch_size,\n                         shuffle=False,\n                         num_workers=CFG_exp018.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in CFG_exp018.trn_fold:\n    model = CustomModel_exp018(CFG_exp018, config_path=CFG_exp018.config_path, pretrained=False)\n    state = torch.load(os.path.join(CFG_exp018.path,\"model\" , f\"{CFG_exp018.model.replace('/', '-')}_fold{fold}_best.pth\"),\n                       map_location=torch.device('cpu'))\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    predictions.append(prediction)\n    del model, state, prediction; gc.collect()\n    torch.cuda.empty_cache()\npredictions_exp018 = np.mean(predictions, axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# exp032","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# tokenizer\n# ====================================================\nCFG_exp032.tokenizer = AutoTokenizer.from_pretrained(CFG_exp032.path+'tokenizer/')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Model\n# ====================================================\nclass CustomModel_exp032(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n        self._init_weights(self.fc)\n        self.attention = nn.Sequential(\n            nn.Linear(self.config.hidden_size, self.config.hidden_size),\n            nn.LayerNorm(self.config.hidden_size),\n            nn.GELU(),\n            nn.Linear(self.config.hidden_size, 1),\n            nn.Softmax(dim=1)\n        )\n        self.fc_dropout1 = nn.Dropout(0.1)\n        self.fc_dropout2 = nn.Dropout(0.2)\n        self.fc_dropout3 = nn.Dropout(0.3)\n        self.fc_dropout4 = nn.Dropout(0.4)\n        self.fc_dropout5 = nn.Dropout(0.5)\n        \n        self._init_weights(self.attention)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        # feature = torch.mean(last_hidden_states, 1)\n        weights = self.attention(last_hidden_states)\n        feature = torch.sum(weights * last_hidden_states, dim=1)\n        return feature\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n\n        feature1 = self.fc_dropout1(feature)\n        feature2 = self.fc_dropout2(feature)\n        feature3 = self.fc_dropout3(feature)\n        feature4 = self.fc_dropout4(feature)\n        feature5 = self.fc_dropout5(feature)\n\n        feature_all = (feature1+feature2+feature3+feature4+feature5)/5\n        output = self.fc(feature_all)\n        \n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(CFG_exp032, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG_exp032.batch_size,\n                         shuffle=False,\n                         num_workers=CFG_exp032.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor fold in CFG_exp032.trn_fold:\n    model = CustomModel_exp032(CFG_exp032, config_path=CFG_exp032.config_path, pretrained=False)\n    state = torch.load(os.path.join(CFG_exp032.path,\"model\" , f\"{CFG_exp032.model.replace('/', '-')}_fold{fold}_best.pth\"),\n                       map_location=torch.device('cpu'))\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    predictions.append(prediction)\n    del model, state, prediction; gc.collect()\n    torch.cuda.empty_cache()\npredictions_exp032 = np.mean(predictions, axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"# ensamble\n\nMMscaler = MinMaxScaler()\n\npredictions_exp026 = MMscaler.fit_transform(predictions_exp026.reshape(-1,1)).reshape(-1)\npredictions_exp020 = MMscaler.fit_transform(predictions_exp020.reshape(-1,1)).reshape(-1)\npredictions_exp011 = MMscaler.fit_transform(predictions_exp011.reshape(-1,1)).reshape(-1)\npredictions_exp018 = MMscaler.fit_transform(predictions_exp018.reshape(-1,1)).reshape(-1)\npredictions_exp032 = MMscaler.fit_transform(predictions_exp032.reshape(-1,1)).reshape(-1)\n\nuse_models =[\n    \"../input/pppm-baseline-026\",\n    \"../input/pppm-baseline-020\",\n    \"../input/pppm-baseline-018\",\n    \"../input/pppm-baseline-011\",\n    \"../input/pppm-baseline-032\"\n]\n\nprediction_lists = [\n    predictions_exp026,\n    predictions_exp020,\n    predictions_exp018,\n    predictions_exp011,\n    predictions_exp032\n]\n\ndata_dir = {}\nfor model_name, p in zip(use_models, prediction_lists):\n    data_dir[model_name.split(\"/\")[-1]] = p\n    \n\nprediction_df = pd.DataFrame(data_dir)\n# final_predictions = []\n# # 回帰モデル\n# for model in models:\n#     prediction = model.predict(prediction_df)\n#     final_predictions.append(prediction)\n# final_prediction = np.mean(final_predictions,axis=0)\n# final_prediction\n# #final_predictions =  predictions_deberta * round(deberta_weight, 2) + predictions_bert_patents * round(bert_patents_weight, 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# stacking","metadata":{}},{"cell_type":"code","source":"display(prediction_df)\na","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_data = pd.DataFrame(final_predictions).T\ntest_data = pd.DataFrame(final_predictions).T\n\ntest_data.columns = [f\"pred{i}\" for i in range(len(test_data.columns))]\nfeatures = test_data.columns\ntest_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class stacking_CFG:\n    max_grad_norm=1000\n    gradient_accumulation_steps=1\n    hidden_size=256\n    dropout=0.3\n    lr=1e-4\n    batch_size=128\n    epochs=50\n    weight_decay=1e-5\n    n_fold = 5\n    features = test.columns\n    model_dir = \"../input/5model-linear-stacking/5model_kfsky_linear/\"\n    model_path = None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class StackingDataset(Dataset):\n    def __init__(self, df, features, labels=None, model_name=None):\n        self.df = df[features].values\n        if model_name == \"2dcnn\":\n            # [N, Models, Labels, Channel] -> [N, Channel, Models, Labels]\n            self.df = self.df.reshape(-1, len(features), 1, 1)\n            self.df = self.df.transpose(0,3,1,2)\n        if not labels is None:\n            self.labels = labels.values\n        else:\n            self.labels = labels\n        \n    def __len__(self, ):\n        return len(self.df)\n    \n    \n    def __getitem__(self, item):\n        inputs =  torch.FloatTensor(self.df[item]).float()\n        \n        if self.labels is None:\n            return inputs\n        labels = torch.tensor(self.labels[item]).float()\n        return inputs, labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1dcnn","metadata":{}},{"cell_type":"code","source":"class oneDCNN(nn.Module):\n\n        def __init__(self, num_features, num_targets, hidden_size, cfg=None):\n            self. model_name=\"1dcnn\"\n            super(oneDCNN, self).__init__()\n            hidden_size = 1024\n            cha_1 = 256\n            cha_2 = 512\n            cha_3 = 512\n\n            cha_1_reshape = int(hidden_size/cha_1)\n            cha_po_1 = int(hidden_size/cha_1/2)\n            cha_po_2 = int(hidden_size/cha_1/2/2) * cha_3\n\n            self.cha_1 = cha_1\n            self.cha_2 = cha_2\n            self.cha_3 = cha_3\n            self.cha_1_reshape = cha_1_reshape\n            self.cha_po_1 = cha_po_1\n            self.cha_po_2 = cha_po_2\n\n            self.batch_norm1 = nn.BatchNorm1d(num_features)\n            self.dropout1 = nn.Dropout(0.1)\n            self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n\n            self.batch_norm_c1 = nn.BatchNorm1d(cha_1)\n            self.dropout_c1 = nn.Dropout(0.1)\n            self.conv1 = nn.utils.weight_norm(nn.Conv1d(cha_1,cha_2, kernel_size = 5, stride = 1, padding=2,  bias=False),dim=None)\n\n            self.ave_po_c1 = nn.AdaptiveAvgPool1d(output_size = cha_po_1)\n\n            self.batch_norm_c2 = nn.BatchNorm1d(cha_2)\n            self.dropout_c2 = nn.Dropout(0.1)\n            self.conv2 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_2, kernel_size = 3, stride = 1, padding=1, bias=True),dim=None)\n\n            self.batch_norm_c2_1 = nn.BatchNorm1d(cha_2)\n            self.dropout_c2_1 = nn.Dropout(0.3)\n            self.conv2_1 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_2, kernel_size = 3, stride = 1, padding=1, bias=True),dim=None)\n\n            self.batch_norm_c2_2 = nn.BatchNorm1d(cha_2)\n            self.dropout_c2_2 = nn.Dropout(0.2)\n            self.conv2_2 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_3, kernel_size = 5, stride = 1, padding=2, bias=True),dim=None)\n\n            self.max_po_c2 = nn.MaxPool1d(kernel_size=4, stride=2, padding=1)\n\n            self.flt = nn.Flatten()\n\n            self.batch_norm3 = nn.BatchNorm1d(cha_po_2)\n            self.dropout3 = nn.Dropout(0.2)\n            self.dense3 = nn.utils.weight_norm(nn.Linear(cha_po_2, num_targets))\n\n        def forward(self, x):\n\n            x = self.batch_norm1(x)\n            x = self.dropout1(x)\n            x = F.celu(self.dense1(x), alpha=0.06)\n\n            x = x.reshape(x.shape[0],self.cha_1,\n                          self.cha_1_reshape)\n\n            x = self.batch_norm_c1(x)\n            x = self.dropout_c1(x)\n            x = F.relu(self.conv1(x))\n\n            x = self.ave_po_c1(x)\n\n            x = self.batch_norm_c2(x)\n            x = self.dropout_c2(x)\n            x = F.relu(self.conv2(x))\n            x_s = x\n\n            x = self.batch_norm_c2_1(x)\n            x = self.dropout_c2_1(x)\n            x = F.relu(self.conv2_1(x))\n\n            x = self.batch_norm_c2_2(x)\n            x = self.dropout_c2_2(x)\n            x = F.relu(self.conv2_2(x))\n            x =  x * x_s\n\n            x = self.max_po_c2(x)\n\n            x = self.flt(x)\n\n            x = self.batch_norm3(x)\n            x = self.dropout3(x)\n            x = self.dense3(x)\n\n            return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference_fn(test_loader, model, device):\n\n    model.eval()\n    preds = []\n\n    for step, (x) in enumerate(test_loader):\n\n        x = x.to(device, non_blocking=True)\n\n        with torch.no_grad():\n            pred = model(x)\n\n        preds.append(pred.sigmoid().detach().cpu().numpy())\n#         preds.append(pred.tanh().detach().cpu().numpy())\n\n    preds = np.concatenate(preds)\n\n    return preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.notebook import tqdm\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import io\nimport zipfile\ndef run_single_nn(cfg, test, features, device, fold_num, seed, MODEL,save_dir,like_sklearn=False):\n    seed_everything(seed=seed)\n    \n    if like_sklearn:\n        model = MODEL()\n        model_path = io.BytesIO()\n        with zipfile.ZipFile(model_path, \"w\") as z:\n            z.write(f\"{save_dir}tabnet_fold{fold_num}_seed{seed}/model_params.json\", arcname=\"model_params.json\")\n            z.write(f\"{save_dir}tabnet_fold{fold_num}_seed{seed}/network.pt\", arcname=\"network.pt\")\n        model.load_model(model_path)\n\n        predictions = model.predict(test[features].values)\n    else:\n        model = MODEL(\n                num_features=len(features), num_targets=1, hidden_size=cfg.hidden_size, cfg=cfg\n        )\n\n        model.load_state_dict(torch.load(f\"{save_dir}{model.model_name}_fold{fold_num}_seed{seed}.pth\",map_location=\"cpu\"), )\n        model.to(device, non_blocking=True)\n\n        test_dataset = StackingDataset(test, features, None, model.model_name)\n\n        test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False, \n                                  num_workers=4, pin_memory=False, drop_last=False)\n\n\n        # inference\n        predictions = inference_fn(test_loader, model, device)     \n    return predictions\n                          \n\ndef run_kfold_nn(cfg, test, features, device, n_fold, seed,MODEL, save_dir, like_sklearn=False):\n    oof = np.zeros((len(test), 1))\n    for fold_num in range(n_fold):\n        LOGGER.info(f\"fold {fold_num}\")\n        _oof = run_single_nn(cfg, test, features, device, fold_num, seed, MODEL,save_dir,like_sklearn)\n        oof += _oof\n    return oof/n_fold","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stacking_CFG.model_path = stacking_CFG.model_dir+\"1dcnn/\"\npredictions = np.zeros((len(test), 1))\nSEED = [42, 1999, 2022]\nfor i, seed in enumerate(SEED):\n    print(seed)\n    _pred = run_kfold_nn(stacking_CFG, test_data, features, device,\n                        n_fold=5, seed=seed, MODEL=oneDCNN, save_dir=stacking_CFG.model_path,\n                         )\n    predictions += _pred/len(SEED)\n    \npredictions_1dcnn = predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2dcnn","metadata":{}},{"cell_type":"code","source":"class twoDCNN(nn.Module):#input5\n    def __init__(self, num_features, num_targets, hidden_size, cfg):\n        self.model_name = \"2dcnn\"\n        self.num_features = num_features\n        self.num_targets = num_targets\n        super(twoDCNN, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(3, 1), bias=False)\n        self.relu1 = nn.ReLU()\n        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(2, 1), bias=False)\n        self.relu2 = nn.ReLU()\n        \n        # self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(2, 1), bias=False)\n        # self.relu3 = nn.ReLU()\n        # self.conv4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 1), bias=False)\n        # self.relu4 = nn.ReLU()\n        \n#         self.conv5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(2, 1), bias=False)\n#         self.relu5 = nn.ReLU()\n#         self.conv6 = nn.Conv2d(in_channels=128, out_channels=1024, kernel_size=(1, 1), bias=False)\n#         self.relu6 = nn.ReLU()\n            \n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(in_features=32, out_features=512)\n        # self.fc1 = nn.Linear(in_features=80, out_features=512)\n        self.relu3 = nn.ReLU()\n        self.fc2 = nn.Linear(in_features=512, out_features=num_targets)\n        \n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.relu1(x)\n        x = self.conv2(x)\n        x = self.relu2(x)\n        \n#         x = self.conv3(x)\n#         x = self.relu3(x)\n#         x = self.conv4(x)\n#         x = self.relu4(x)\n        \n#         x = self.conv5(x)\n#         x = self.relu5(x)\n#         x = self.conv6(x)\n#         x = self.relu6(x)\n\n        x = self.flatten(x)\n        x = self.fc1(x)\n        x = self.relu3(x)\n        x = self.fc2(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stacking_CFG.model_path = stacking_CFG.model_dir+\"2dcnn/\"\npredictions = np.zeros((len(test), 1))\nSEED = [42, 1999, 2022]\nfor i, seed in enumerate(SEED):\n    print(seed)\n    _pred = run_kfold_nn(stacking_CFG, test_data, features, device,\n                        n_fold=5, seed=seed, MODEL=twoDCNN, save_dir=stacking_CFG.model_path,\n                         )\n    predictions += _pred/len(SEED)\n    \npredictions_2dcnn = predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MLP","metadata":{}},{"cell_type":"code","source":"class StackingMLP(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size, cfg):\n        super().__init__()\n        self.model_name = \"mlp\"\n        cfg.num_features = num_features\n        cfg.hidden_size = hidden_size\n    \n        self.mlp = nn.Sequential(\n            nn.BatchNorm1d((num_features)),\n            nn.Dropout(cfg.dropout),\n            nn.utils.weight_norm(nn.Linear((cfg.num_features), cfg.hidden_size)),\n            nn.PReLU(),\n\n            \n            nn.BatchNorm1d(cfg.hidden_size),\n            nn.Dropout(cfg.dropout),\n            nn.utils.weight_norm(nn.Linear(cfg.hidden_size, cfg.hidden_size)),\n            nn.PReLU(),\n\n                        \n            nn.BatchNorm1d(cfg.hidden_size),\n            nn.Dropout(cfg.dropout),\n            nn.utils.weight_norm(nn.Linear(cfg.hidden_size, num_targets)),\n\n        )\n        \n        self.shallow_mlp = nn.Sequential(\n        \n                          nn.Linear((cfg.num_features), cfg.hidden_size),\n                          nn.BatchNorm1d(cfg.hidden_size),\n                          nn.Dropout(cfg.dropout),\n                          nn.PReLU(),\n                          nn.Linear(cfg.hidden_size, cfg.hidden_size),\n                          nn.BatchNorm1d(cfg.hidden_size),\n                          nn.Dropout(cfg.dropout),\n                          nn.PReLU(),\n                          nn.Linear(cfg.hidden_size, num_targets),\n                          )\n\n\n    def forward(self, x):\n        # if \"shallow\" in  stacking_exp_name:\n        x = self.shallow_mlp(x)\n        # else:\n        #     x = self.mlp(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stacking_CFG.model_path = stacking_CFG.model_dir+\"mlp/\"\npredictions = np.zeros((len(test), 1))\nstacking_exp_name=\"shallow\"\nSEED = [42, 1999, 2022]\nfor i, seed in enumerate(SEED):\n    print(seed)\n    _pred = run_kfold_nn(stacking_CFG, test_data, features, device,\n                        n_fold=5, seed=seed, MODEL=StackingMLP, save_dir=stacking_CFG.model_path,\n                         )\n    predictions += _pred/len(SEED)\npredictions_mlp = predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# GCN","metadata":{}},{"cell_type":"code","source":"import typing as tp\n\nclass MLP(nn.Module):\n    \"\"\"Stacked Dense layers\"\"\"\n    \n    def __init__(\n        self, n_features_list: tp.List[int], use_tail_as_out: bool=False,\n        drop_rate: float=0.0, use_bn: bool=False, use_wn: bool=False,\n        activ:str=\"relu\", block_name: str=\"LBAD\"\n    ):\n        \"\"\"\"\"\"\n        super(MLP, self).__init__()\n        n_layers = len(n_features_list) - 1\n        block_class = {\n            \"LBAD\": LBAD, \"BDLA\": BDLA, \"LABD\": LABD}[block_name]\n        layers = []\n        for i in range(n_layers):\n            in_feats, out_feats = n_features_list[i: i + 2]\n            if i == n_layers - 1 and use_tail_as_out:\n                if block_name in [\"BDLA\"]:\n                    layer = block_class(in_feats, out_feats, drop_rate, use_bn,  use_wn, \"identity\")\n                else:\n                    layer = nn.Linear(in_feats, out_feats)\n                    if use_wn:\n                        layer = nn.utils.weight_norm(layer)\n            else:\n                layer = block_class(in_feats, out_feats, drop_rate, use_bn,  use_wn, activ)\n            layers.append(layer)\n                \n        self.layers = nn.Sequential(*layers)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\"\"\"\n        return self.layers(x)\n\n\nclass CNNStacking1d(nn.Module):\n    \"\"\"1D-CNN for Stacking.\"\"\"\n    \n    def __init__(\n        self, n_models: int,\n        n_channels_list: tp.List[int], use_bias: bool=False,\n        kwargs_head: tp.Dict={},\n    ):\n        \"\"\"\"\"\"\n        super(CNNStacking1d, self).__init__()\n        self.n_conv_layers = len(n_channels_list) - 1\n        for i in range(self.n_conv_layers):\n            in_ch = n_channels_list[i]\n            out_ch = n_channels_list[i + 1]\n            layer = nn.Sequential(\n                nn.Conv1d(\n                    in_ch, out_ch, kernel_size=3, stride=1, padding=0, bias=use_bias),\n                # nn.BatchNorm1d(out_ch),\n                nn.ReLU(inplace=True))\n            setattr(self, \"conv{}\".format(i + 1), layer)\n        \n        kwargs_head[\"n_features_list\"][0] = (n_models - 2 * self.n_conv_layers) * n_channels_list[-1]\n        self.head = MLP(**kwargs_head)\n    \n    def forward(self, x: torch.FloatTensor) -> torch.Tensor:\n        \"\"\"\"\"\"\n        bs = x.shape[0]\n        h = x  # shape: (bs, n_classes, n_models)\n        for i in range(self.n_conv_layers):\n            h = getattr(self, \"conv{}\".format(i + 1))(h)\n            \n        h = torch.reshape(h, (bs, -1))\n        h = self.head(h)\n        return h\n    \n    \n    \ndef get_activation(activ_name: str=\"relu\"):\n    \"\"\"\"\"\"\n    act_dict = {\n        \"relu\": nn.ReLU(),\n        \"tanh\": nn.Tanh(),\n        \"sigmoid\": nn.Sigmoid(),\n        \"identity\": nn.Identity()}\n    if activ_name in act_dict:\n        return act_dict[activ_name]\n    elif re.match(r\"^htanh\\_\\d{4}$\", activ_name):\n        bound = int(activ_name[-4:]) / 1000\n        return nn.Hardtanh(-bound, bound)\n    else:\n        raise NotImplementedError\n\nclass LBAD(nn.Module):\n    \"\"\"Linear (-> BN) -> Activation (-> Dropout)\"\"\"\n    \n    def __init__(\n        self, in_features: int, out_features: int, drop_rate: float=0.0,\n        use_bn: bool=False, use_wn: bool=False, activ: str=\"relu\"\n    ):\n        \"\"\"\"\"\"\n        super(LBAD, self).__init__()\n        layers = [nn.Linear(in_features, out_features)]\n        if use_wn:\n            layers[0] = nn.utils.weight_norm(layers[0])\n        \n        if use_bn:\n            layers.append(nn.BatchNorm1d(out_features))\n        \n        layers.append(get_activation(activ))\n        \n        if drop_rate > 0:\n            layers.append(nn.Dropout(drop_rate))\n        \n        self.layers = nn.Sequential(*layers)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\"\"\"\n        return self.layers(x)\n    \n    \nclass BDLA(nn.Module):\n    \"\"\"(BN -> Dropout ->) Linear -> Activation\"\"\"\n    \n    def __init__(\n        self, in_features: int, out_features: int, drop_rate: float=0.0,\n        use_bn: bool=False, use_wn: bool=False, activ: str=\"relu\"\n    ):\n        \"\"\"\"\"\"\n        super(BDLA, self).__init__()\n        layers = []\n        if use_bn:\n            layers.append(nn.BatchNorm1d(in_features))\n            \n        if drop_rate > 0:\n            layers.append(nn.Dropout(drop_rate))\n        \n        layers.append(nn.Linear(in_features, out_features))\n        if use_wn:\n            layers[-1] = nn.utils.weight_norm(layers[-1])\n            \n        layers.append(get_activation(activ))\n        \n        self.layers = nn.Sequential(*layers)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\"\"\"\n        return self.layers(x)\n    \n\nclass LABD(nn.Module):\n    \"\"\"Linear -> Activation (-> BN -> Dropout) \"\"\"\n    \n    def __init__(\n        self, in_features: int, out_features: int, drop_rate: float=0.0,\n        use_bn: bool=False, use_wn: bool=False, activ: str=\"relu\"\n    ):\n        \"\"\"\"\"\"\n        super(LABD, self).__init__()\n        layers = [nn.Linear(in_features, out_features), get_activation(activ)]\n        \n        if use_wn:\n            layers[0] = nn.utils.weight_norm(layers[0])\n        \n        if use_bn:\n            layers.append(nn.BatchNorm1d(out_features))\n        \n        if drop_rate > 0:\n            layers.append(nn.Dropout(drop_rate))\n        \n        self.layers = nn.Sequential(*layers)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\"\"\"\n        return self.layers(x)\n# # for GCNs\ndef vector_wise_matmul(X: torch.Tensor, W: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    See input matrixes X as bags of vectors, and multiply corresponding weight matrices by vector.\n    \n    Args:\n        X: Input Tensor, shape: (batch_size, **n_vectors**, in_features)\n        W: Weight Tensor, shape: (**n_vectors**, out_features, in_features)\n    \"\"\"\n\n    X = torch.transpose(X, 0, 1)  # shape: (n_vectors, batch_size, in_features)\n    W = torch.transpose(W, 1, 2)  # shape: (n_vectors, in_features, out_features)\n    H = torch.matmul(X, W)        # shape: (n_vectors, batch_size, out_features)\n    H = torch.transpose(H, 0, 1)  # shape: (batch_size, n_vectors, out_features)\n    \n    return H\n\n\ndef vector_wise_shared_matmul(X: torch.Tensor, W: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    See input matrixes X as bags of vectors, and multiply **shared** weight matrices.\n    \n    Args:\n        X: Input Tensor, shape: (batch_size, **n_vectors**, in_features)\n        W: Weight Tensor, shape: (out_features, in_features)\n    \"\"\"\n    # W = torch.transpose(W, 0, 1)  # shape: (in_features, out_features)\n    # H = torch.matmul(X, W)        # shape: (batch_size, n_vectors, out_features)\n    \n    H = nn.functional.linear(X, W)  # shape: (batch_size, n_vectors, out_features)\n    \n    return H\ndef _calculate_fan_in_and_fan_out_for_vwl(tensor) -> tp.Tuple[int]:\n    \"\"\"\n    Input tensor: (n_vectors, out_features, in_features) or (out_features, in_features)\n    \"\"\"\n    dimensions = tensor.dim()\n    if dimensions < 2:\n        raise ValueError(\"Fan in and fan out can not be computed for tensor with fewer than 2 dimensions\")\n\n    fan_in = tensor.size(-1)\n    fan_out = tensor.size(-2)\n\n    return fan_in, fan_out\n    \n\ndef _calculate_correct_fan_for_vwl(tensor, mode) -> int:\n    \"\"\"\"\"\"\n    mode = mode.lower()\n    valid_modes = ['fan_in', 'fan_out']\n    if mode not in valid_modes:\n        raise ValueError(\"Mode {} not supported, please use one of {}\".format(mode, valid_modes))\n\n    fan_in, fan_out = _calculate_fan_in_and_fan_out_for_vwl(tensor)\n    return fan_in if mode == 'fan_in' else fan_out\n\n\ndef kaiming_uniform_for_vwl(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu'):\n    \"\"\"\"\"\"\n    fan = _calculate_correct_fan_for_vwl(tensor, mode)\n    gain = nn.init.calculate_gain(nonlinearity, a)\n    std = gain / np.sqrt(fan)\n    bound = np.sqrt(3.0) * std  # Calculate uniform bounds from standard deviation\n    with torch.no_grad():\n        return tensor.uniform_(-bound, bound)\nclass VectorWiseLinear(nn.Module):\n    \"\"\"\n    For mini batch which have several matrices,\n    see as these matrixes as bags of vectors, and multiply weight matrices by vector.\n    \n    input    X: (batch_size, **n_vectors**, in_features)\n    weight W: (**n_vector**, out_features, in_features)\n    output  Y: (batch_size, **n_vectors**, out_features)\n\n    **Note**: For simplicity, bias is not described.\n    \n    X and W are can be seen as below.\n    X: [\n            [vec_{ 1, 1}, vec_{ 1, 2}, ... vec_{ 1, n_vectors}],\n            [vec_{ 2, 1}, vec_{ 2, 2}, ... vec_{ 2, n_vectors}],\n                                            .\n                                            .\n            [vec_{bs, 1}, vec_{bs, 2}, ... vec_{bs, n_vectors}]\n        ]\n    W: [\n            Mat_{1}, Mat_{2}, ... , Mat_{n_vectors}\n        ]\n    Then Y is calclauted as:\n    Y: [\n        [ Mat_{1} vec_{ 1, 1}, Mat_{2} vec_{ 1, 2}, ... Mat_{n_vectors} vec_{ 1, n_vectors}],\n        [ Mat_{1} vec_{ 2, 1}, Mat_{2} vec_{ 2, 2}, ... Mat_{n_vectors} vec_{ 2, n_vectors}],\n        .\n        .\n        [ Mat_{1} vec_{bs, 1}, Mat_{2} vec_{bs, 2}, ... Mat_{n_vectors} vec_{bs, n_vectors}],\n    ]\n    \"\"\"\n    \n    def __init__(\n        self,\n        in_features: int, out_features: int, n_vectors: int,\n        bias: bool=True, weight_shared: bool=True\n    ) -> None:\n        \"\"\"Initialize.\"\"\"\n        super(VectorWiseLinear, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.n_vectors = n_vectors\n        self.weight_shared = weight_shared\n        \n        if self.weight_shared:\n            self.weight = nn.Parameter(\n                torch.Tensor(self.out_features, self.in_features)).to(device)\n            self.matmul_func = vector_wise_shared_matmul\n        else:\n            self.weight = nn.Parameter(\n                torch.Tensor(self.n_vectors, self.out_features, self.in_features))\n            self.matmul_func = vector_wise_matmul\n            \n        if bias:\n            self.bias = nn.Parameter(torch.Tensor(out_features))\n        else:\n            self.register_parameter('bias', None)\n            \n        self.reset_parameters()\n        \n    def reset_parameters(self) -> None:\n        \"\"\"Initialize weight and bias.\"\"\"\n        kaiming_uniform_for_vwl(self.weight, a=np.sqrt(5))\n        if self.bias is not None:\n            fan_in, _ = _calculate_fan_in_and_fan_out_for_vwl(self.weight)\n            bound = 1 / np.sqrt(fan_in)\n            nn.init.uniform_(self.bias, -bound, bound)\n             \n    def forward(self, X: torch.Tensor) -> torch.Tensor:\n        \"\"\"Forward.\"\"\"\n        H = self.matmul_func(X, self.weight)\n        if self.bias is not None:\n            H = H + self.bias\n        \n        return H\nclass GraphConv(nn.Module):\n    \"\"\"Basic Graph Convolution Layer.\"\"\"\n    \n    def __init__(\n        self, \n        in_channels: int, out_channels: int, n_nodes: int, shrare_msg: bool=True,\n        model_self: bool=True, share_model_self: bool=True,\n        bias: bool=True, share_bias: bool=True\n    ) -> None:\n        \"\"\"Intialize.\"\"\"\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.n_nodes = n_nodes\n        self.model_self = model_self\n        super(GraphConv, self).__init__()\n        \n        # # message\n        self.msg = VectorWiseLinear(\n            in_channels, out_channels, n_nodes, False, shrare_msg)\n\n        # # self-modeling\n        if model_self:\n            self.model_self = VectorWiseLinear(\n                in_channels, out_channels, n_nodes, False, share_model_self)\n        \n        # # bias\n        if bias:\n            if share_bias:\n                self.bias = nn.Parameter(torch.Tensor(out_channels))\n            else:\n                self.bias = nn.Parameter(torch.Tensor(n_nodes, out_channels))\n            bound = 1 / np.sqrt(out_channels)\n            nn.init.uniform_(self.bias, -bound, bound)\n        else:\n            self.register_parameter('bias', None)\n            \n    \n    def forward(self, X: torch.Tensor, A: torch.Tensor, W: torch.Tensor=None) -> torch.Tensor:\n        \"\"\"Forward.\n        \n        Args:\n            X: (batch_size, n_nodes, n_channels)\n                Array which represents bags of vectors.\n                X[:, i, :] are corresponded to feature vectors of node i.\n            A: (batch_size, n_nodes, n_nodes)\n                Array which represents adjacency matrices.\n                A[:, i, j] are corresponded to weights (scalar) of edges from node j to node i.\n            W: (batch_size, n_nodes, n_nodes)\n                Array which represents weight matrices between nodes.\n        \"\"\"\n        if W is not None:\n            A = A * W  # shape: (batch_size, n_nodes, n_nodes)\n        \n        # # update message\n        M = X  #  shape: (batch_size, n_nodes, in_channels)\n        # # # send message\n        M = self.msg(M)  # shape: (batch_size, n_nodes, out_channels)\n        # # # aggregate\n        M = torch.matmul(A, M)  # shape: (batch_size, n_nodes, out_channels)\n            \n        # # update node\n        # # # self-modeling\n        H = M\n        if self.model_self:\n            H = H + self.model_self(X)\n        if self.bias is not None:\n            H = H + self.bias\n        \n        return H","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GCN(nn.Module):\n    \"\"\"GCN for Stacking.\"\"\"\n    \n    # def __init__(\n    #     self, n_classes: int,\n    #     n_channels_list: tp.List[int],\n    #     add_self_loop: bool=False,\n    #     kwargs_head: tp.Dict={},\n    # ):\n    def __init__(self, num_features, num_targets, hidden_size, cfg):\n        self.model_name = \"GCN\"\n        n_classes = 1\n        n_channels_list =  [5, 16, 16, 16, 16, 16,16]#[8, 16, 16, 16, 16, 16, 16,]\n        add_self_loop = True\n        kwargs_head = {\n            \"n_features_list\": [-1, 768, 1],# [-1, 2048, 1]\n            \"use_tail_as_out\": True,\n            \"drop_rate\": 0.8,\n            \"use_bn\": False,\n            \"use_wn\": True,\n            \"block_name\": \"LABD\",\n        }\n        \n        \n        super().__init__()\n        self.n_conv_layers = len(n_channels_list) - 1\n        for i in range(self.n_conv_layers):\n            in_ch = n_channels_list[i]\n            out_ch = n_channels_list[i + 1]\n            # layer = CustomGraphConv(in_ch, out_ch, n_classes)\n            layer = GraphConv(\n                in_ch, out_ch, n_classes,\n                shrare_msg=False, share_model_self=False, share_bias=False)\n            setattr(self, \"conv{}\".format(i + 1), layer)\n        \n        self.relu = nn.ReLU(inplace=True)\n        if add_self_loop:\n            adj_mat = torch.ones(n_classes, n_classes) / n_classes\n        else:\n            adj_mat = (1 - torch.eye(n_classes, n_classes)) / (n_classes - 1) \n        self.register_buffer(\"A\", adj_mat.float())\n               \n        kwargs_head[\"n_features_list\"][0] = n_classes * n_channels_list[-1]\n        self.head = MLP(**kwargs_head)\n    \n    def forward(self, X: torch.FloatTensor) -> torch.Tensor:\n        \"\"\"\"\"\"\n        X = X.unsqueeze(1)\n        bs, n_classes = X.shape[:2]\n        H = X  # shape: (bs, n_classes, n_models)\n        for i in range(self.n_conv_layers):\n            H = getattr(self, \"conv{}\".format(i + 1))(H, self.A[None, ...])\n            H = self.relu(H)\n        \n        h = torch.reshape(H, (bs, -1))\n        h = self.head(h)\n        return h","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stacking_CFG.model_path = stacking_CFG.model_dir+\"gcn/\"\npredictions = np.zeros((len(test), 1))\n\nSEED = [42, 1999, 2022]\ndevice = torch.device(\"cpu\")\nfor i, seed in enumerate(SEED):\n    print(seed)\n    _pred = run_kfold_nn(stacking_CFG, test_data, features, device,\n                        n_fold=5, seed=seed, MODEL=GCN, save_dir=stacking_CFG.model_path,\n                         )\n    predictions += _pred/len(SEED)\n\npredictions_gcn = predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# tabnet","metadata":{}},{"cell_type":"code","source":"# os.system('python -m pip install --no-index --find-links=../input/pytorchtabnet/pytorch-tabnet  pytorch-tabnet')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\nstacking_CFG.model_path = stacking_CFG.model_dir+\"tabnet/\"\npredictions = np.zeros((len(test), 1))\n\nSEED = [42, 1999, 2022]\ndevice = torch.device(\"cpu\")\nfor i, seed in enumerate(SEED):\n    print(seed)\n    _pred = run_kfold_nn(stacking_CFG, test_data, features, device,\n                        n_fold=5, seed=seed, MODEL=TabNetRegressor, save_dir=stacking_CFG.model_path, like_sklearn=True\n                         )\n    predictions += _pred/len(SEED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_tabnet = predictions\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_df = pd.DataFrame(np.concatenate([\n    predictions_gcn,predictions_1dcnn, predictions_2dcnn, predictions_mlp, predictions_tabnet\n], axis=1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_predictions = []\nfor model in models:\n    prediction = model.predict(prediction_df)\n    final_predictions.append(prediction)\nfinal_prediction = np.mean(final_predictions,axis=0)\n\n# submission[\"score\"] = final_prediction\n# submission[[\"id\", \"score\"]].to_csv(f\"submission.csv\",index=False)\n\nfinal_prediction","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# postprocess\n","metadata":{}},{"cell_type":"code","source":"len(preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dataset\noof_df_exp020 = pd.read_pickle(os.path.join(CFG_exp020.path, \"preds\", \"oof_df.pkl\"))\npostprocess_df = pd.merge(train_df[[\"id\"]], oof_df_exp020, on=\"id\", how=\"left\")\npostprocess_df = postprocess_df.drop([\"pred\", \"fold\", \"score_map\", \"anchor_map\"],axis=1)\npostprocess_df[\"pred\"] = preds\n\n\nprint(get_score(postprocess_df[\"score\"], postprocess_df[\"pred\"]))\n\n## 残差を算出\npostprocess_df[\"res\"] = postprocess_df[\"score\"] - postprocess_df[\"pred\"]\npostprocess_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom contextlib import contextmanager\nfrom time import time\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD, NMF, LatentDirichletAllocation\nfrom sklearn.pipeline import Pipeline\nimport re\nfrom bs4 import BeautifulSoup\n\nclass BaseBlock(object):\n    def fit(self, input_df, y=None):\n        return self.transform(input_df)\n    \n    def transform(self, input_df):\n        raise NotImplementedError()\n\n        \nclass ContinuousBlock(BaseBlock):\n    def __init__(self, column):\n        self.column = column\n        \n    def transform(self, input_df):\n        return input_df[self.column].copy()\n\n\n# 大元の実装では、whole_df(train_df+test_df)であるが、今回は裏側にいる\n# これはいいのか？\nclass CountEncodingBlock(BaseBlock):\n    def __init__(self, column, whole_df):\n        self.column = column\n        self.whole_df = whole_df\n    \n    def transform(self, input_df):\n        output_df = pd.DataFrame()\n        vc = self.whole_df[self.column].value_counts()\n        output_df[self.column] = input_df[self.column].map(vc)\n        \n        return output_df.add_prefix(\"CE_\")\n\n\nclass OneHotEncodingBlock(BaseBlock):\n    def __init__(self, column, count_limit: int):\n        self.column = column\n        self.count_limit = count_limit\n\n    def fit(self, input_df, y=None):\n        vc = input_df[self.column].dropna().value_counts()\n        cats_ = vc[vc > self.count_limit].index\n        self.cats_ = cats_\n        return self.transform(input_df)\n\n    def transform(self, input_df):\n        x = pd.Categorical(input_df[self.column], categories=self.cats_)\n        output_df = pd.get_dummies(x, dummy_na=False)\n        output_df.columns = output_df.columns.to_list()\n        return output_df.add_prefix(f'OHE_{self.column}=')\n    \n\nclass LabelEncodingBlock(BaseBlock):\n    def __init__(self, column, whole_df):\n        self.column = column\n        self.whole_df = whole_df\n        self.le = LabelEncoder()\n        \n    def fit(self, input_df, y=None):\n        self.le.fit(self.whole_df[self.column].fillna(\"nan\"))\n        return self.transform(input_df)\n    \n    def transform(self, input_df):\n        c = self.column\n        output_df = input_df.copy()\n        output_df[c] = self.le.transform(input_df[c].fillna(\"nan\")).astype(int)\n        output_df = output_df[[c]]\n        return output_df.add_prefix(\"LE_\")\n    \n    \n    class WrapperBlock(BaseBlock):\n        def __init__(self,function):\n            self.function = function\n            \n        def transform(self, input_df):\n            return self.function(input_df)\n\n\nclass AggregationBlock(BaseBlock):\n    def __init__(self,\n                 whole_df: pd.DataFrame,\n                 key: str,\n                 agg_column: str,\n                 agg_funcs: [\"mean\"],\n                 fillna=None):\n        self.whole_df = whole_df\n        self.key = key\n        self.agg_column = agg_column\n        self.agg_funcs = agg_funcs\n        self.fillna = fillna\n\n    def fit(self, input_df):\n        return self.transform(input_df)\n\n    def transform(self, input_df):\n        if self.fillna:\n            self.whole_df[self.agg_column] = self.whole_df[self.agg_column].fillna(self.fillna)\n\n        self.group_df = self.whole_df.groupby(self.key).agg({self.agg_column: self.agg_funcs}).reset_index()\n        column_names = [f'GP_{self.agg_column}@{self.key}_{agg_func}' for agg_func in self.agg_funcs]\n\n        self.group_df.columns = [self.key] + column_names\n        output_df = pd.merge(input_df[self.key], self.group_df, on=self.key, how=\"left\").drop(columns=[self.key])\n        return output_df\n\n\nclass StringLengthBlock(BaseBlock):\n    def __init__(self, column):\n        self.column = column\n\n    def transform(self, input_df):\n        output_df = pd.DataFrame()\n        output_df[self.column] = input_df[self.column].str.len()\n        return output_df.add_prefix(\"StringLength_\")\n\n\nclass WordCountBlock(BaseBlock):\n    def __init__(self, column: str):\n        self.column = column\n\n    def transform(self, input_df):\n        output_df = pd.DataFrame()\n        output_df[f\"WORDCOUNT_{self.column}\"] = input_df[self.column].astype(str).map(lambda x: len(x.split()))\n\n        return output_df\n    \n\ndef text_cleaning(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n\n    return text\n\n    \nclass TfidfBlock(BaseBlock):\n    def __init__(self, column: str, whole_df: pd.DataFrame, decomposition: str, n_compose: int):\n        self.column = column\n        self.whole_df = whole_df\n        self.decomposition = decomposition\n        self.n_compose = n_compose\n\n    def fit(self, input_df, y=None):\n        master_df = self.whole_df\n        text = self.whole_df[self.column].fillna(\"\")\n\n        if self.decomposition == \"svd\":\n            self.pipeline_ = Pipeline([\n                (\"tfidf\", TfidfVectorizer(max_features=50000)),\n                (\"svd\", TruncatedSVD(n_components=self.n_compose, random_state=71))\n            ])\n\n        elif self.decomposition == \"NMF\":\n            self.pipeline_ = Pipeline([\n                (\"tfidf\", TfidfVectorizer(max_features=50000)),\n                (\"NMF\", NMF(n_components=self.n_compose, random_state=71))\n            ])\n        elif self.decomposition == \"LDA\":\n            self.pipeline_ = Pipeline([\n                (\"tfidf\", TfidfVectorizer(max_features=50000)),\n                (\"NMF\", LatentDirichletAllocation(n_components=self.n_compose, random_state=71))\n            ])\n        else:\n            self.pipeline_ = Pipeline([\n                (\"tfidf\", TfidfVectorizer(max_features=50000)),\n                (\"svd\", TruncatedSVD(n_components=self.n_compose, random_state=71))\n            ])\n\n        self.pipeline_.fit(text)\n\n        return self.transform(input_df)\n\n    def transform(self, input_df):\n        text = input_df[self.column].fillna(\"\")\n        z = self.pipeline_.transform(text)\n\n        out_df = pd.DataFrame(z)\n        return out_df.add_prefix(f'{self.column}_tfidf_{self.decomposition}_')\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@contextmanager\ndef timer(logger=None, format_str='{:.3f}[s]', prefix=None, suffix=None):\n    if prefix: format_str = str(prefix) + format_str\n    if suffix: format_str = format_str + str(suffix)\n    start = time()\n    yield\n    d = time() - start\n    out_str = format_str.format(d)\n    if logger:\n        logger.info(out_str)\n    else:\n        print(out_str)\n\n\ndef get_function(block, is_train):\n    s = mapping = {\n        True: 'fit',\n        False: 'transform'\n    }.get(is_train)\n    return getattr(block, s)\n\n\ndef to_feature(input_df,\n               blocks,\n               is_train=False):\n    out_df = pd.DataFrame()\n\n    for block in tqdm(blocks, total=len(blocks)):\n        func = get_function(block, is_train)\n\n        with timer(prefix='create ' + str(block) + ' '):\n            _df = func(input_df)\n        assert len(_df) == len(input_df), func.__name__\n        out_df = pd.concat([out_df, _df], axis=1)\n    return reduce_mem_usage(out_df)\n\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    for col in df.columns:\n        if verbose:\n            print(col)\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'\n              .format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL_NAME = '../input/uspppm-debertv3large-5folds/uspppm_0'\n\n# トークナイザとモデルのロード\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n#tokenizer = DebertaV2TokenizerFast.from_pretrained(MODEL_NAME)\nmodel = AutoModel.from_pretrained(MODEL_NAME)\nmodel = model.cuda()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## testデータの読み込み\n_test_df = pd.read_csv(\"../input/us-patent-phrase-to-phrase-matching/test.csv\")\ntitles = pd.read_csv('../input/cpc-codes/titles.csv')\n_test_df = _test_df.merge(titles, left_on='context', right_on='code')\n_test_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_test_df['input'] = _test_df['title']+' '+_test_df['anchor']\n_test_df['input2'] = _test_df[\"input\"] + \" \" + _test_df[\"target\"]\n\nmax_length = 128\nsentence_vectors = []\n\nfor n in range(len(_test_df)):\n    # 記事から文章を抜き出し、符号化を行う。\n    text = _test_df[\"input2\"][n]\n    encoding = tokenizer(\n        text, \n        max_length=max_length, \n        padding='max_length', \n        truncation=True, \n        return_tensors='pt'\n    )\n    encoding = { k: v.cuda() for k, v in encoding.items() } \n    attention_mask = encoding['attention_mask']\n\n    # 文章ベクトルを計算\n    # BERTの最終層の出力を平均を計算する。（ただし、[PAD]は除く。）\n    with torch.no_grad():\n        output = model(**encoding)\n        last_hidden_state = output.last_hidden_state \n        averaged_hidden_state = \\\n            (last_hidden_state*attention_mask.unsqueeze(-1)).sum(1) \\\n            / attention_mask.sum(1, keepdim=True) \n\n    # 文章ベクトルとラベルを追加\n    sentence_vectors.append(averaged_hidden_state[0].cpu().numpy())\n\nsentence_vectors = np.vstack(sentence_vectors)\nsentence_vectors","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"deberta_emb_ts = pd.DataFrame(sentence_vectors)\ndeberta_emb_ts = deberta_emb_ts.add_prefix(\"deberta_v3_large_\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"deberta_emb_ts = pd.concat([_test_df[[\"id\"]], deberta_emb_ts],axis=1)\ntest = pd.merge(test,deberta_emb_ts,on=\"id\", how=\"left\")\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"postprocess_df.head(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 後処理用に新規でdfを起こしておく\nnew_train = postprocess_df.drop(\"text\", axis=1)\nnew_test = test.drop(\"text\", axis=1)\n\nnew_test[\"pred\"] = final_prediction\n\n# 前処理（context）\nnew_train[\"context2\"] = new_train[\"context\"].str[0]\nnew_train[\"context3\"] = new_train[\"context\"].str[1:].astype(str)\n\nnew_test[\"context2\"] = new_test[\"context\"].str[0]\nnew_test[\"context3\"] = new_test[\"context\"].str[1:].astype(str)\n\nwhole_df = pd.concat([new_train, new_test])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"process_blocks = [\n    *[ContinuousBlock(c) for c in [\n        \"pred\",\n    ]],\n    *[CountEncodingBlock(c, whole_df=whole_df) for c in [\n        \"context\",\n        \"context2\",\n        \"context3\"\n    ]],\n    *[LabelEncodingBlock(c, whole_df=whole_df) for c in [\n        \"context\"\n    ]],\n    *[StringLengthBlock(c) for c in [\n        \"anchor\",\n        \"target\",\n        \"context_text\"\n    ]],\n    *[WordCountBlock(c) for c in [\n        \"anchor\",\n        \"target\",\n        \"context_text\"\n    ]],\n    TfidfBlock(\"context_text\" ,whole_df=whole_df, decomposition=\"svd\", n_compose=150)\n    ]\n\n# 今回は残差が目的変数\ntrain_y = postprocess_df[\"res\"]\ntrain_x = to_feature(new_train, process_blocks, is_train=True)\ntest_x = to_feature(new_test, process_blocks)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x.head(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_x.head(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\ndef fit_lgbm(X, y, cv, params=None, verbose: int=50):\n    metric_func = mean_squared_error\n    \n    if params is None:\n        params = dict()\n        \n    models = []\n    \n    oof_pred = np.zeros_like(y, dtype=np.float)\n    \n    for i, (tr_idx, val_idx) in enumerate(cv):\n        x_train, y_train = X[tr_idx], y[tr_idx]\n        x_valid, y_valid = X[val_idx], y[val_idx]\n        \n        clf = lgb.LGBMRegressor(**params)\n        \n        with timer(prefix=\"fit_fold{}\".format(i+1)):\n            clf.fit(x_train, y_train,\n                   eval_set=[(x_valid,y_valid)],\n                   early_stopping_rounds=verbose,\n                   verbose=-1)\n        \n        pred_i = clf.predict(x_valid)\n        oof_pred[val_idx] = pred_i\n        models.append(clf)\n        \n        print(f'Fold {i} RMSE: {metric_func(y_valid, pred_i)**.5 :.4f}')\n    score = metric_func(y, oof_pred)**.5 \n    print('FINISHED | Whole RMSE: {:.4f}'.format(score))\n    return oof_pred, models","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedGroupKFold\nlgm_params = {  \n    \"n_estimators\": 20000,\n    \"objective\": 'rmse',\n    \"learning_rate\": 0.01,\n    \"num_leaves\": 32,\n    \"random_state\": 71,\n    \"n_jobs\": -1,\n    \"importance_type\": \"gain\",\n    'colsample_bytree': 0.7,\n    \"max_depth\":5,\n    }\n\npostprocess_df['score_map'] = postprocess_df['score'].map({0.00: 0, 0.25: 1, 0.50: 2, 0.75: 3, 1.00: 4})\nencoder = LabelEncoder()\npostprocess_df['anchor_map'] = encoder.fit_transform(postprocess_df['anchor'])\n\nfold_lgbm = Fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=71)\ncv_lgbm = list(fold_lgbm.split(postprocess_df, postprocess_df['score_map']))\n\noof_lgbm, models_lgbm = fit_lgbm(train_x.values, train_y, cv_lgbm , params=lgm_params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 追加後処理\n# https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching/discussion/327288\n# 意味なかったので使用せず\ndef update_label(pred):\n    if -0.01 < pred <= .01:\n        return .0\n    elif .24 <= pred <= .26:\n        return .25\n    elif .49 <= pred <= .51:\n        return .5\n    elif .74 <= pred <= .76:\n        return .75\n    elif .99 <= pred <= 1.01:\n        return 1.0\n\n    return pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 出力結果が補正値になるので、これを結果に当てはめる\npostprocess_df[\"res_oof\"] = oof_lgbm\npostprocess_df[\"pp_pred\"] = postprocess_df[\"res_oof\"] + postprocess_df[\"pred\"]\n\n\nbefore_pp = get_score(postprocess_df[\"score\"], postprocess_df[\"pred\"])\nafter_pp = get_score(postprocess_df[\"score\"], postprocess_df[\"pp_pred\"])\nprint(f\"before_pp：{round(before_pp,4)}\")\nprint(f\"after_pp：{round(after_pp,4)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(12,4))\n\nax1 = fig.add_subplot(1, 2, 1)\nsns.scatterplot(postprocess_df[\"score\"], postprocess_df[\"pred\"],ax=ax1)\nax1.set_ylim(-0.1, 1.1)\n\nax2 = fig.add_subplot(1, 2, 2)\nsns.scatterplot(postprocess_df[\"score\"], postprocess_df[\"pp_pred\"],ax=ax2)\nax2.set_ylim(-0.1, 1.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_importance(models, feat_train_df):\n    \"\"\"\n    args:\n        models:\n            List of lightGBM models\n        feat_train_df:\n            学習時に使った DataFrame\n    \"\"\"\n    feature_importance_df = pd.DataFrame()\n    for i, model in enumerate(models):\n        _df = pd.DataFrame()\n        _df['feature_importance'] = model.feature_importances_\n        _df['column'] = feat_train_df.columns\n        _df['fold'] = i + 1\n        feature_importance_df = pd.concat([feature_importance_df, _df], \n                                          axis=0, ignore_index=True)\n\n    order = feature_importance_df.groupby('column')\\\n        .sum()[['feature_importance']]\\\n        .sort_values('feature_importance', ascending=False).index[:50]\n\n    fig, ax = plt.subplots(figsize=(8, max(6, len(order) * .25)))\n    sns.boxenplot(data=feature_importance_df, \n                  x='feature_importance', \n                  y='column', \n                  order=order, \n                  ax=ax, \n                  palette='viridis', \n                  orient='h')\n    ax.tick_params(axis='x', rotation=90)\n    ax.set_title('Importance')\n    ax.grid()\n    fig.tight_layout()\n    return fig, ax","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = visualize_importance(models_lgbm, train_x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(8, 8))\nsns.histplot(train_y, label='Train_y', ax=ax, color='black')\nsns.histplot(oof_lgbm, label='Out Of Fold', ax=ax, color='C1')\nax.legend()\nax.grid()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_prediction_pp = np.array([model.predict(test_x.values) for model in models_lgbm])\nfinal_prediction_pp = np.mean(final_prediction_pp, axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#final_prediction = np.where(final_prediction < 0, 0, final_prediction)\n#final_prediction = np.where(final_prediction > 1, 1, final_prediction)\nsubmission['score'] = final_prediction + final_prediction_pp\n\n# post process2\nsame_ids = test[test[\"anchor\"]==test[\"target\"]][\"id\"].tolist()\nif len(same_ids) > 0:\n    submission.loc[submission[\"id\"].isin(same_ids), \"score\"] = 1.0 # 同じphaseが入っているものは1に変換する\n    \ndisplay(submission.head())\nsubmission[['id', 'score']].to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}